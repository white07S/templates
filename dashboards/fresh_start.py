# from pathlib import Path
# from typing import Dict, Any, List, Tuple
# import json
# import os
# from dotenv import load_dotenv

# from agno.agent import Agent, RunResponse
# from agno.models.deepseek import DeepSeek
# from agno.tools.python import PythonTools


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: copy code files into logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def add_file_content(logs: dict, filename: str, log_key: str):
#     """Attach the *text* of `filename` (or a placeholder) to logs."""
#     try:
#         with open(filename, "r") as f:
#             logs[log_key] = f.read()
#     except FileNotFoundError:
#         logs[log_key] = f"{filename} not produced."


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLEAN AGENT PERSONAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eda_persona = """
# You are Dr. Sarah Chen, a quantitative analyst specializing in financial time series.
# ALLOWED LIBRARIES: pandas, numpy, matplotlib, seaborn, statsmodels, scipy
# TASKS: Data loading, quality assessment, time series decomposition, stationarity tests, correlation analysis, outlier detection, data cleaning
# """

# fe_persona = """
# You are Dr. Michael Rodriguez, a feature engineering expert in financial forecasting.
# ALLOWED LIBRARIES: pandas, numpy, scipy, statsmodels
# TASKS: Technical indicator creation, volatility features, lagged features, rolling statistics, feature selection, scaling
# """

# model_persona = """
# You are Dr. Emily Watson, a machine learning engineer specializing in financial predictions.
# ALLOWED LIBRARIES: sklearn, xgboost, lightgbm, catboost, numpy, pandas, joblib (DONT USE ANYTHING EXCEPT THESE LIBRARIES)
# TASKS: Model selection, time series cross-validation, ensemble modeling, hyperparameter tuning, model persistence
# """

# eval_persona = """
# You are Dr. James Park, a model evaluation specialist in quantitative finance.
# ALLOWED LIBRARIES: sklearn, matplotlib, seaborn, numpy, pandas, scipy
# TASKS: Prediction metrics, residual analysis, error distribution, model diagnostics, performance benchmarking
# """


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ instantiate agents â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eda_agent = Agent(
#     name="EDA_Agent",
#     model=DeepSeek(),
#     description=eda_persona,
#     tools=[PythonTools()],
# )

# feature_engineering_agent = Agent(
#     name="FeatureEngineering_Agent", 
#     model=DeepSeek(),
#     description=fe_persona,
#     tools=[PythonTools()],
# )

# modeling_agent = Agent(
#     name="Modeling_Agent",
#     model=DeepSeek(),
#     description=model_persona,
#     tools=[PythonTools()],
# )

# evaluation_agent = Agent(
#     name="Evaluation_Agent",
#     model=DeepSeek(),
#     description=eval_persona,
#     tools=[PythonTools()],
# )


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ logging factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def generate_submission_log(
#     agent_run_details: List[Tuple[str, Agent, str, str]],
#     log_filename: str = "submission_log.json",
#     script_key_map: Dict[str, str] | None = None,
# ) -> Dict[str, Any]:
#     """
#     Build `submission_log.json` from a list of
#     (agent_name, agent_instance, runtime_prompt, output_log).
#     """
#     if script_key_map is None:
#         script_key_map = {
#             "EDA.py":     "EDA_Script",
#             "FEATURE.py": "FeatureEngineering_Script", 
#             "MODEL.py":   "Modeling_Script",
#             "EVAL.py":    "Evaluation_Script",
#         }

#     logs: Dict[str, Any] = {}

#     # â”€â”€ per-agent blocks â”€â”€
#     for name, agent_obj, run_prompt, output in agent_run_details:
#         logs[name] = {
#             "prompt": f"{agent_obj.description}\n\n{run_prompt.strip()}",
#             "output_log": output,
#         }

#     # â”€â”€ attach generated scripts â”€â”€
#     for filename, log_key in script_key_map.items():
#         add_file_content(logs, filename, log_key)

#     # â”€â”€ write to disk â”€â”€
#     with open(log_filename, "w") as f:
#         json.dump(logs, f, indent=4)

#     return logs


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def run_full_pipeline(
#     train_csv: str,
#     val_csv: str,
#     test_csv: str,
# ) -> List[Tuple[str, Agent, str, str]]:
#     """
#     Executes the 4-agent workflow and returns a list of
#     (agent_name, agent_instance, runtime_prompt, output_log).
#     """
#     run_details: List[Tuple[str, Agent, str, str]] = []

#     # ========== EDA ==========
#     eda_prompt = f"""
#     CRITICAL: Write executable Python code ONLY - no markdown blocks. Use ONLY: pandas, numpy, matplotlib, seaborn, statsmodels.
#     TASK: Comprehensive financial time series analysis
#     DATASETS: {train_csv} (train), {val_csv} (validation), {test_csv} (test)
    
#     REQUIREMENTS:
#     1. Load datasets with date parsing and set as index
#     2. Conduct thorough analysis:
#        - Missing value heatmap with seaborn
#        - Stationarity tests (ADF, KPSS) for all numeric columns
#        - Time series decomposition (trend, seasonality, residuals)
#        - Correlation matrices (Pearson and Spearman)
#        - Volatility clustering analysis (GARCH model diagnostics)
#        - Distribution plots (kurtosis, skewness) for returns
#     3. Data cleaning:
#        - Forward-fill missing values then drop remaining NAs
#        - Cap outliers at 3 standard deviations using robust z-scores
#        - Validate temporal consistency across splits
#     4. Save cleaned data as: train_clean.csv, val_clean.csv, test_clean.csv
#     5. Generate and save all plots as PNG files
#     6. Final script MUST be saved as EDA.py
#     7. Print file creation summary at end
    
#     IMPORTANT: Use plt.savefig() for plots - NEVER plt.show()
#     """
    
#     eda_output = ""
#     for chunk in eda_agent.run(eda_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         eda_output += chunk.content
#     run_details.append(("EDA_Agent", eda_agent, eda_prompt, eda_output))

#     # ========== Feature Engineering ==========
#     fe_prompt = f"""
#     CRITICAL: Write executable Python code ONLY - no markdown blocks. Use ONLY: pandas, numpy, statsmodels.
#     TASK: Advanced financial feature engineering
#     INPUT: train_clean.csv, val_clean.csv, test_clean.csv
    
#     REQUIREMENTS:
#     1. Create comprehensive feature set:
#        - Technical indicators: Ichimoku Cloud, Keltner Channels, Elder Ray Index
#        - Volatility features: Garman-Klass, Rogers-Satchell, Yang-Zhang estimators
#        - Fourier transforms for seasonality extraction
#        - Wavelet transforms for multi-resolution analysis
#        - Lagged returns (1, 3, 5, 10 periods)
#        - Rolling features: Z-score, quantile(0.25), entropy(20)
#     2. Strictly prevent look-ahead bias (shift operations only)
#     3. Feature selection:
#        - Remove low-variance features (<0.1)
#        - Eliminate highly correlated pairs (|Ï| > 0.95)
#     4. Scale features using RobustScaler
#     5. Save datasets: train_features.csv, val_features.csv, test_features.csv
#     6. Final script MUST be saved as FEATURE.py
#     7. Print file creation summary at end
    
#     IMPORTANT: Date column must be excluded from feature sets
#     """
    
#     fe_output = ""
#     for chunk in feature_engineering_agent.run(fe_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         fe_output += chunk.content
#     run_details.append(("FeatureEngineering_Agent",
#                         feature_engineering_agent,
#                         fe_prompt,
#                         fe_output))

#     # ========== Modeling ==========
#     modeling_prompt = f"""
#     CRITICAL: Write executable Python code ONLY - no markdown blocks. Use ONLY: sklearn, xgboost, lightgbm, catboost, joblib.
#     TASK: Ensemble modeling with time-series validation
#     INPUT: train_features.csv, val_features.csv
#     TARGET: Target_return
#     PERFORMANCE GOAL: RMSE â‰¤ 0.0011
    
#     REQUIREMENTS:
#     1. Implement two-stage modeling:
#        Stage 1: Train diverse base models:
#          - XGBoost 
#          - LightGBM with focal loss
#          - CatBoost with Ordered boosting
#          - ElasticNet with recursive feature elimination
#        Stage 2: Meta-model (LinearRegression) blending predictions
#     2. Validation strategy:
#        - TimeSeriesSplit (n_splits=5)
#        - Walk-forward validation with expanding window
#     3. Hyperparameter tuning:
#        - Bayesian optimization for tree-based models
#        - Grid search for linear models
#     4. Final model:
#        - Retrain on combined train+validation data
#        - Save ensemble as model.pkl using joblib
#     5. Prepare test data (X_test.csv, y_test.csv) without date column
#     6. Final script MUST be saved as MODEL.py
#     7. Print file creation summary at end
#     """
    
#     modeling_output = ""
#     for chunk in modeling_agent.run(modeling_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         modeling_output += chunk.content
#     run_details.append(("Modeling_Agent",
#                         modeling_agent,
#                         modeling_prompt,
#                         modeling_output))

#     # ========== Evaluation ==========
#     evaluation_prompt = f"""
#     CRITICAL: Write executable Python code ONLY - no markdown blocks. Use ONLY: sklearn, matplotlib, seaborn, numpy.
#     TASK: Comprehensive model evaluation
#     INPUT: model.pkl, X_test.csv, y_test.csv
#     PRIMARY METRIC: RMSE (target â‰¤ 0.0011)
    
#     REQUIREMENTS:
#     1. Calculate metrics:
#        - Primary: RMSE (must be â‰¤ 0.0011)
#        - Secondary: MAE, RÂ², Information Coefficient
#        - Additional: Max Error, MAPE, Theil's U
#     2. Advanced diagnostics:
#        - Residual autocorrelation (Ljung-Box test)
#        - Error distribution (Jarque-Bera normality test)
#        - Cumulative prediction error plot
#        - Quantile-Quantile plot of residuals
#     3. Critical output:
#        - Save RMSE to MSFT_Score.txt in EXACT format: "RMSE: <value>"
#        - Generate model performance report (text summary)
#     4. Final script MUST be saved as EVAL.py
#     5. Print file creation summary at end
    
#     IMPORTANT: Use plt.savefig() for plots - NEVER plt.show()
#     """
    
#     evaluation_output = ""
#     for chunk in evaluation_agent.run(evaluation_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         evaluation_output += chunk.content
#     run_details.append(("Evaluation_Agent",
#                         evaluation_agent,
#                         evaluation_prompt,
#                         evaluation_output))

#     return run_details


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# if __name__ == "__main__":
#     load_dotenv()  # Load environment variables
    
#     base_dir = Path(".")
#     train_csv = "train.csv"
#     val_csv   = "val.csv" 
#     test_csv  = "test.csv"

#     missing = [p for p in (train_csv, val_csv, test_csv) if not Path(p).exists()]
#     if missing:
#         print("Missing required files:")
#         for p in missing:
#             print(f"  - {p}")
#         raise SystemExit(1)

#     print("ðŸš€ Launching Advanced Financial Prediction Pipeline")
#     print("ðŸ” Stage 1: Intelligent EDA with Financial Diagnostics")
#     print("ðŸ§© Stage 2: Advanced Feature Engineering")
#     print("ðŸ¤– Stage 3: Ensemble Modeling with Meta-Learning")
#     print("ðŸ“Š Stage 4: Rigorous Model Evaluation\n")
    
#     agent_runs = run_full_pipeline(
#         str(train_csv), str(val_csv), str(test_csv))
    
#     # Build submission log (automatically includes scripts)
#     submission_logs = generate_submission_log(agent_runs)

#     # Verify critical files
#     required_files = ["EDA.py", "FEATURE.py", "MODEL.py", "EVAL.py", "MSFT_Score.txt"]
#     missing = [f for f in required_files if not Path(f).exists()]
    
#     if missing:
#         print("\nâŒ Pipeline incomplete - missing files:")
#         for f in missing:
#             print(f"  - {f}")
#     else:
#         print("\nâœ… Pipeline successfully completed!")
#         with open("MSFT_Score.txt", "r") as f:
#             print(f"ðŸ“Š Final RMSE: {f.read().strip()}")
#         print("ðŸ“ Full execution log: submission_log.json")


# from pathlib import Path
# from typing import Dict, Any, List, Tuple
# import json
# import os
# from dotenv import load_dotenv

# from agno.agent import Agent, RunResponse
# from agno.models.openai import OpenAIChat
# from agno.tools.python import PythonTools


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SIMPLE MEMORY SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # Use file-based simple memory storage for inter-agent communication
# INSIGHTS_FILE = "agent_insights.json"

# def store_agent_insights(agent_name: str, insights: Dict[str, Any]):
#     """Store agent insights in a simple JSON file for cross-agent coordination."""
#     try:
#         if os.path.exists(INSIGHTS_FILE):
#             with open(INSIGHTS_FILE, "r") as f:
#                 all_insights = json.load(f)
#         else:
#             all_insights = {}
        
#         all_insights[agent_name] = insights
        
#         with open(INSIGHTS_FILE, "w") as f:
#             json.dump(all_insights, f, indent=2)
#     except Exception as e:
#         print(f"Warning: Could not store insights: {e}")

# def get_previous_insights() -> Dict[str, Any]:
#     """Retrieve insights from previous agents for informed decision making."""
#     try:
#         if os.path.exists(INSIGHTS_FILE):
#             with open(INSIGHTS_FILE, "r") as f:
#                 return json.load(f)
#         return {}
#     except Exception as e:
#         print(f"Warning: Could not load insights: {e}")
#         return {}


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: copy code files into logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def add_file_content(logs: dict, filename: str, log_key: str):
#     """Attach the *text* of `filename` (or a placeholder) to logs."""
#     try:
#         with open(filename, "r") as f:
#             logs[log_key] = f.read()
#     except FileNotFoundError:
#         logs[log_key] = f"{filename} not produced."


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ADVANCED COMPETITION-WINNING AGENT PERSONAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# eda_persona = """
# You are Dr. Sarah Chen, a Senior Quantitative Analyst with 15+ years at top-tier hedge funds (Citadel, Two Sigma, Renaissance Technologies). You specialize in financial time series analysis with a track record of generating alpha through sophisticated data insights.

# COMPETITION CONTEXT: This is an LLM-judged financial prediction competition targeting RMSE â‰¤ 0.0011. Your analysis forms the foundation for the entire pipeline's success.

# TECHNICAL EXPERTISE:
# - Advanced statistical analysis and time series decomposition
# - Market microstructure and regime detection
# - Volatility modeling (GARCH, EGARCH, FIGARCH)
# - Risk-adjusted performance metrics and drawdown analysis
# - Cross-asset correlation and spillover effects

# ALLOWED LIBRARIES: pandas, numpy, matplotlib, seaborn, statsmodels, scipy

# CRITICAL REQUIREMENTS FOR 200+ LINES OF SUBSTANTIAL CODE:
# 1. Implement comprehensive data quality framework (outlier detection, missing data analysis, temporal consistency checks)
# 2. Advanced time series diagnostics (stationarity tests, structural breaks, regime detection)
# 3. Multi-timeframe volatility analysis with GARCH modeling
# 4. Cross-sectional and time-series correlation analysis
# 5. Feature stability assessment across different market conditions
# 6. Automated data cleaning with financial domain knowledge
# 7. Risk factor decomposition and principal component analysis
# 8. Performance benchmarking against market indices

# MANDATORY FINANCIAL ANALYSIS COMPONENTS:
# - Rolling Sharpe ratio and information ratio calculations
# - Maximum drawdown and Value-at-Risk analysis
# - Return distribution analysis (skewness, kurtosis, tail risk)
# - Autocorrelation and partial autocorrelation analysis
# - Seasonal pattern detection and holiday effects
# - Market regime identification using Hidden Markov Models
# - Cross-asset spillover effects analysis
# - Feature importance ranking for downstream modeling

# REASONING REQUIREMENT: Document every analytical decision with financial rationale. Explain how each analysis contributes to alpha generation and risk management.
# """

# fe_persona = """
# You are Dr. Michael Rodriguez, Lead Feature Engineering Scientist at a premier quantitative hedge fund with expertise in alpha signal generation. You've developed proprietary features that consistently generate 2+ Sharpe ratio strategies.

# COMPETITION CONTEXT: Your feature engineering directly impacts the final RMSE. Target is â‰¤ 0.0011 - your features must capture subtle market patterns while avoiding overfitting.

# TECHNICAL MASTERY:
# - Advanced technical indicators and momentum signals
# - Volatility surface modeling and regime-dependent features
# - Cross-asset arbitrage signals and cointegration relationships
# - Alternative data integration and nowcasting indicators
# - Feature neutralization and orthogonalization techniques

# ALLOWED LIBRARIES: pandas, numpy, scipy, statsmodels, sklearn (preprocessing only)

# MANDATORY 200+ LINES COMPREHENSIVE FEATURE ENGINEERING:

# 1. TECHNICAL INDICATORS SUITE (50+ lines):
#    - Multi-timeframe momentum indicators (RSI, MACD, Williams %R across 5, 10, 20, 50 periods)
#    - Volatility-based features (Bollinger Bands, ATR, Keltner Channels)
#    - Volume-based signals (OBV, Volume-Price Trend, Accumulation/Distribution)
#    - Trend strength indicators (ADX, Parabolic SAR, Ichimoku components)

# 2. VOLATILITY MODELING FEATURES (40+ lines):
#    - GARCH-based volatility forecasts
#    - Realized volatility estimators (Garman-Klass, Rogers-Satchell, Yang-Zhang)
#    - Volatility of volatility (second-order moments)
#    - Regime-dependent volatility clustering detection

# 3. CROSS-ASSET SIGNALS (30+ lines):
#    - Rolling correlation coefficients with market indices
#    - Cointegration-based mean reversion signals
#    - Beta stability and factor exposures
#    - Sector rotation and style momentum indicators

# 4. ALTERNATIVE FEATURES (40+ lines):
#    - Fourier transform features for cyclical patterns
#    - Wavelet decomposition for multi-resolution analysis
#    - Entropy-based complexity measures
#    - Information-theoretic feature selection

# 5. FEATURE ENGINEERING PIPELINE (40+ lines):
#    - Automated lag generation with optimal window selection
#    - Feature neutralization against common factors
#    - Orthogonalization using Gram-Schmidt process
#    - Robust scaling with outlier-resistant methods

# CROSS-AGENT INTEGRATION: Build upon EDA insights about data quality, seasonality, and regime changes. Incorporate findings about correlation structures and volatility patterns.

# FINANCIAL REASONING: Every feature must have clear economic interpretation. Document alpha generation hypothesis and expected market conditions where each feature provides edge.
# """

# model_persona = """
# You are Dr. Emily Watson, Head of Machine Learning at a top-tier systematic trading firm. You've built production models managing $10B+ AUM with consistent alpha generation. Your expertise lies in ensemble methods and time-series aware machine learning.

# COMPETITION CONTEXT: This is the critical stage - your ensemble must achieve RMSE â‰¤ 0.0011. Focus on sophisticated stacking methods and financial domain-specific modeling techniques.

# MODELING EXPERTISE:
# - Advanced ensemble methods (stacking, blending, Bayesian model averaging)
# - Time-series aware cross-validation and walk-forward analysis
# - Financial loss functions and risk-adjusted optimization
# - Concept drift detection and adaptive modeling
# - Production-grade model deployment and monitoring

# ALLOWED LIBRARIES: sklearn, xgboost, lightgbm, catboost, numpy, pandas, joblib, optuna

# MANDATORY 200+ LINES COMPETITION-WINNING ENSEMBLE:

# 1. DIVERSE BASE LEARNER PORTFOLIO (60+ lines):
#    - XGBoost with custom financial objective function
#    - LightGBM with focal loss for handling class imbalance
#    - CatBoost with ordered boosting for time series
#    - ElasticNet with recursive feature elimination
#    - SVR with RBF kernel for non-linear patterns
#    - Extra Trees with bootstrap sampling

# 2. ADVANCED STACKING ARCHITECTURE (50+ lines):
#    - Level-1: Train diverse base models with time-series CV
#    - Level-2: Meta-model ensemble (LinearRegression + Ridge + Lasso)
#    - Level-3: Final blending with dynamic weights based on recent performance
#    - Out-of-fold predictions to prevent overfitting
#    - Temporal consistency constraints

# 3. FINANCIAL-SPECIFIC OPTIMIZATION (40+ lines):
#    - Custom loss function combining RMSE with directional accuracy
#    - Time-decay weighting for recent observations
#    - Volatility-adjusted sample weights
#    - Risk-aware hyperparameter tuning with Sharpe ratio constraints

# 4. SOPHISTICATED VALIDATION FRAMEWORK (30+ lines):
#    - Purged time series cross-validation
#    - Walk-forward analysis with expanding/rolling windows
#    - Gap periods to prevent lookahead bias
#    - Monte Carlo cross-validation for robustness testing

# 5. PRODUCTION-GRADE DEPLOYMENT (20+ lines):
#    - Model serialization with version control
#    - Ensemble weight optimization using recent performance
#    - Prediction confidence intervals and uncertainty quantification
#    - Automated model retraining triggers

# CROSS-AGENT COORDINATION: Leverage EDA insights about data regimes and feature engineering outputs about signal strength. Adapt model complexity based on feature stability analysis.

# PERFORMANCE TARGET: Demonstrate clear path to RMSE â‰¤ 0.0011 through methodical ensemble construction and rigorous validation.
# """

# eval_persona = """
# You are Dr. James Park, Chief Risk Officer and Model Validation specialist with 20+ years experience in quantitative finance. You've overseen model validation for billion-dollar trading strategies and understand regulatory requirements for financial AI systems.

# COMPETITION CONTEXT: Final evaluation phase for LLM judge. Your comprehensive analysis must demonstrate model superiority, risk controls, and production readiness.

# VALIDATION EXPERTISE:
# - Comprehensive model diagnostics and stress testing
# - Financial risk metrics and regulatory compliance
# - Performance attribution and factor analysis
# - Real-world deployment considerations
# - Backtesting methodology and statistical significance

# ALLOWED LIBRARIES: sklearn, matplotlib, seaborn, numpy, pandas, scipy, statsmodels

# MANDATORY 200+ LINES COMPREHENSIVE EVALUATION FRAMEWORK:

# 1. PERFORMANCE METRICS SUITE (50+ lines):
#    - Primary: RMSE calculation with confidence intervals
#    - Financial: Sharpe ratio, information ratio, Calmar ratio
#    - Risk: Maximum drawdown, VaR, CVaR, volatility metrics
#    - Directional: Hit ratio, precision/recall for directional moves
#    - Statistical: RÂ², adjusted RÂ², AIC/BIC for model selection

# 2. ADVANCED DIAGNOSTICS (50+ lines):
#    - Residual analysis with autocorrelation tests (Ljung-Box, Durbin-Watson)
#    - Heteroscedasticity tests (Breusch-Pagan, White)
#    - Normality tests (Jarque-Bera, Shapiro-Wilk, Anderson-Darling)
#    - Structural stability tests (Chow test, CUSUM)
#    - Model adequacy assessment

# 3. FINANCIAL RISK ANALYSIS (40+ lines):
#    - Rolling performance metrics with regime analysis
#    - Drawdown duration and recovery time analysis
#    - Tail risk assessment with extreme value theory
#    - Factor exposure analysis and style attribution
#    - Stress testing under different market conditions

# 4. STATISTICAL SIGNIFICANCE TESTING (30+ lines):
#    - Model comparison using Diebold-Mariano test
#    - Bootstrap confidence intervals for all metrics
#    - Cross-validation stability assessment
#    - Out-of-sample significance testing
#    - Economic significance vs statistical significance

# 5. PRODUCTION READINESS ASSESSMENT (30+ lines):
#    - Model interpretability and explainability analysis
#    - Computational efficiency and latency testing
#    - Memory usage and scalability assessment
#    - Error handling and edge case analysis
#    - Regulatory compliance documentation

# CROSS-AGENT SYNTHESIS: Integrate insights from all previous agents. Validate that EDA findings are reflected in model performance, confirm feature engineering effectiveness, and assess model robustness.

# CRITICAL OUTPUT REQUIREMENTS:
# - MSFT_Score.txt with exact format: "RMSE: <value>"
# - Comprehensive performance report demonstrating competitive advantage
# - Risk assessment suitable for institutional deployment
# - Statistical validation proving model superiority

# FINANCIAL REASONING: Every metric must be interpreted from a risk-adjusted returns perspective. Demonstrate that the model not only achieves low RMSE but also provides economically meaningful predictions suitable for real-world trading.
# """


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ instantiate agents without memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eda_agent = Agent(
#     name="EDA_Agent",
#     model=OpenAIChat(id="o3-mini"),
#     description=eda_persona,
#     tools=[PythonTools()]
# )

# feature_engineering_agent = Agent(
#     name="FeatureEngineering_Agent", 
#     model=OpenAIChat(id="o3-mini"),
#     description=fe_persona,
#     tools=[PythonTools()]
# )

# modeling_agent = Agent(
#     name="Modeling_Agent",
#     model=OpenAIChat(id="o3-mini"),
#     description=model_persona,
#     tools=[PythonTools()]
# )

# evaluation_agent = Agent(
#     name="Evaluation_Agent",
#     model=OpenAIChat(id="o3-mini"),
#     description=eval_persona,
#     tools=[PythonTools()]
# )


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ logging factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def generate_submission_log(
#     agent_run_details: List[Tuple[str, Agent, str, str]],
#     log_filename: str = "submission_log.json",
#     script_key_map: Dict[str, str] | None = None,
# ) -> Dict[str, Any]:
#     """
#     Build `submission_log.json` from a list of
#     (agent_name, agent_instance, runtime_prompt, output_log).
#     """
#     if script_key_map is None:
#         script_key_map = {
#             "EDA.py":     "EDA_Script",
#             "FEATURE.py": "FeatureEngineering_Script", 
#             "MODEL.py":   "Modeling_Script",
#             "EVAL.py":    "Evaluation_Script",
#         }

#     logs: Dict[str, Any] = {}

#     # â”€â”€ per-agent blocks â”€â”€
#     for name, agent_obj, run_prompt, output in agent_run_details:
#         logs[name] = {
#             "prompt": f"{agent_obj.description}\n\n{run_prompt.strip()}",
#             "output_log": output,
#         }

#     # â”€â”€ attach generated scripts â”€â”€
#     for filename, log_key in script_key_map.items():
#         add_file_content(logs, filename, log_key)

#     # â”€â”€ write to disk â”€â”€
#     with open(log_filename, "w") as f:
#         json.dump(logs, f, indent=4)

#     return logs


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ advanced pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def run_advanced_pipeline(
#     train_csv: str,
#     val_csv: str,
#     test_csv: str,
# ) -> List[Tuple[str, Agent, str, str]]:
#     """
#     Executes the advanced 4-agent workflow with simple file-based memory integration and 
#     competition-winning strategies.
#     """
#     run_details: List[Tuple[str, Agent, str, str]] = []

#     # Clear previous insights
#     if os.path.exists(INSIGHTS_FILE):
#         os.remove(INSIGHTS_FILE)

#     # ========== ADVANCED EDA WITH FINANCIAL EXPERTISE ==========
#     eda_prompt = f"""
#     CRITICAL MISSION: You are competing for a $100,000 prize in financial prediction. Your EDA analysis forms the foundation for achieving RMSE â‰¤ 0.0011. Every insight you generate will be used by downstream agents.
#     COLUMNS NAME: Date,Close,Volume,Open,High,Low,Target_return
#     DATASETS: {train_csv} (train), {val_csv} (validation), {test_csv} (test)
    
#     MANDATORY DELIVERABLES (Generate 200+ lines of sophisticated financial analysis code):

#     ## PHASE 1: COMPREHENSIVE DATA QUALITY ASSESSMENT (50+ lines)
#     ```python
#     # Advanced data loading with financial-aware parsing
#     # Multi-dimensional missing value analysis with heatmaps
#     # Temporal consistency validation across splits
#     # Outlier detection using financial domain knowledge (3-sigma, IQR, Hampel filter)
#     # Data type optimization for memory efficiency
#     # Holiday and market closure impact analysis
#     ```

#     ## PHASE 2: ADVANCED FINANCIAL TIME SERIES ANALYSIS (80+ lines)
#     ```python
#     # Stationarity testing suite (ADF, KPSS, PP tests) with interpretation
#     # Structural break detection using Chow test and CUSUM
#     # Regime identification with Hidden Markov Models
#     # Volatility clustering analysis and ARCH effects testing
#     # Cross-correlation analysis between assets/features
#     # Seasonal decomposition with financial calendar awareness
#     # Autocorrelation analysis with financial interpretation
#     # Rolling statistics analysis (mean, volatility, skewness, kurtosis)
#     ```

#     ## PHASE 3: RISK AND RETURN ANALYSIS (40+ lines)
#     ```python
#     # Return distribution analysis with financial metrics
#     # Tail risk assessment (VaR, CVaR calculations)
#     # Sharpe ratio evolution and regime-dependent performance
#     # Drawdown analysis and recovery time estimation
#     # Correlation matrix with hierarchical clustering
#     # Factor analysis and principal component decomposition
#     ```

#     ## PHASE 4: ADVANCED VISUALIZATION AND INSIGHTS (30+ lines)
#     ```python
#     # Professional financial charts with multiple subplots
#     # Correlation heatmaps with significance testing
#     # Time series plots with regime highlighting
#     # Distribution plots with statistical overlays
#     # Rolling correlation analysis visualization
#     # Save all plots as high-quality PNG files
#     ```

#     EXECUTION REQUIREMENTS:
#     1. Write executable Python code ONLY - no markdown blocks, DONT USE TRIPLE BACKQUOTES.
#     2. Use ONLY: pandas, numpy, matplotlib, seaborn, statsmodels, scipy
#     3. Forward-fill missing values then drop remaining NAs
#     4. Cap outliers at 3 standard deviations using robust methods
#     5. Save cleaned data as: train_clean.csv, val_clean.csv, test_clean.csv
#     6. Generate comprehensive insights document for next agents
#     7. Final script MUST be saved as EDA.py
#     8. Print detailed summary of findings and data characteristics

#     FINANCIAL REASONING REQUIRED: Document every analytical decision with clear financial rationale. Explain how findings will impact feature engineering and modeling decisions.

#     COMPETITIVE EDGE: Your analysis must uncover subtle patterns that competitors miss. Focus on:
#     - Market microstructure effects
#     - Cross-asset spillover patterns  
#     - Regime-dependent behavior
#     - Volatility forecasting opportunities
#     - Mean reversion vs momentum signals

#     TARGET: Generate insights that lead to RMSE â‰¤ 0.0011 in final predictions.
#     """
    
#     print("ðŸ”¬ Stage 1: Advanced Financial Data Analysis by Dr. Sarah Chen")
#     print("ðŸŽ¯ Target: Uncover alpha-generating patterns for RMSE â‰¤ 0.0011")
    
#     eda_output = ""
#     for chunk in eda_agent.run(eda_prompt, stream=True):
#         if chunk.content is not None:
#             eda_output += chunk.content
    
#     # Store EDA insights for other agents
#     eda_insights = {
#         "stage": "EDA_Complete",
#         "findings": "Advanced financial time series analysis completed",
#         "data_quality": "Assessed and cleaned",
#         "patterns_identified": "Market regimes, volatility clusters, correlation structures"
#     }
#     store_agent_insights("EDA_Agent", eda_insights)
    
#     run_details.append(("EDA_Agent", eda_agent, eda_prompt, eda_output))

#     # ========== ADVANCED FEATURE ENGINEERING ==========
#     previous_insights = get_previous_insights()
    
#     fe_prompt = f"""
#     MISSION CRITICAL: Based on Dr. Chen's EDA insights, engineer features that will achieve RMSE â‰¤ 0.0011. You're competing against the world's best quants.
    
#     PREVIOUS AGENT INSIGHTS: {json.dumps(previous_insights, indent=2)}
    
#     INPUT FILES: train_clean.csv, val_clean.csv, test_clean.csv

#     MANDATORY DELIVERABLES (Generate 200+ lines of advanced feature engineering):

#     ## PHASE 1: TECHNICAL INDICATORS MASTERY (60+ lines)
#     ```python
#     # MOMENTUM INDICATORS (20 lines)
#     # - Multi-timeframe RSI (5, 10, 20, 50 periods) with divergence detection
#     # - MACD with signal line crossovers and histogram analysis
#     # - Williams %R with overbought/oversold regime identification
#     # - Stochastic oscillator with %K and %D components
#     # - Rate of Change (ROC) across multiple horizons

#     # VOLATILITY INDICATORS (20 lines)  
#     # - Bollinger Bands with squeeze detection and breakout signals
#     # - Average True Range (ATR) with volatility regime classification
#     # - Keltner Channels with trend strength assessment
#     # - Commodity Channel Index (CCI) for cyclical analysis
#     # - Volatility-adjusted momentum indicators

#     # VOLUME INDICATORS (20 lines)
#     # - On-Balance Volume (OBV) with trend confirmation
#     # - Volume-Price Trend (VPT) for smart money tracking
#     # - Accumulation/Distribution Line with buying pressure
#     # - Money Flow Index (MFI) for volume-weighted momentum
#     # - Volume-weighted moving averages (VWMA)
#     ```

#     ## PHASE 2: ADVANCED VOLATILITY MODELING (50+ lines)
#     ```python
#     # REALIZED VOLATILITY ESTIMATORS (25 lines)
#     # - Garman-Klass volatility estimator using OHLC data
#     # - Rogers-Satchell estimator for drift-independent volatility
#     # - Yang-Zhang estimator combining overnight and intraday volatility
#     # - Parkinson volatility using high-low range
#     # - Rolling volatility with adaptive window sizing

#     # VOLATILITY SURFACE FEATURES (25 lines)
#     # - Volatility of volatility (second-order moments)
#     # - Volatility clustering detection using GARCH models
#     # - Regime-dependent volatility with threshold models
#     # - Volatility forecasting using EWMA and RiskMetrics
#     # - Volatility smile and skew indicators
#     ```

#     ## PHASE 3: CROSS-ASSET AND FACTOR SIGNALS (40+ lines)
#     ```python
#     # CORRELATION FEATURES (20 lines)
#     # - Dynamic beta estimation with regime awareness
#     # - Correlation breakdowns and flight-to-quality signals
#     # - Cross-asset momentum spillovers
#     # - Sector rotation strength indicators

#     # FACTOR EXPOSURE FEATURES (20 lines)
#     # - Fama-French factor exposures (market, size, value)
#     # - Momentum and reversal factor loadings
#     # - Quality and profitability factor exposures
#     # - Low volatility factor signals
#     # - Factor-neutral residual returns
#     ```

#     ## PHASE 4: ALTERNATIVE DATA FEATURES (30+ lines)
#     ```python
#     # SPECTRAL ANALYSIS (15 lines)
#     # - Fourier transform for cyclical pattern detection
#     # - Wavelet decomposition for multi-resolution analysis
#     # - Spectral density estimation for frequency domain analysis
#     # - Dominant frequency identification

#     # INFORMATION THEORY FEATURES (15 lines)
#     # - Shannon entropy for complexity measurement
#     # - Mutual information for feature dependency
#     # - Transfer entropy for causality detection
#     # - Permutation entropy for time series analysis
#     ```

#     ## PHASE 5: FEATURE ENGINEERING PIPELINE (20+ lines)
#     ```python
#     # ADVANCED PREPROCESSING
#     # - Optimal lag selection using information criteria
#     # - Feature neutralization against common factors
#     # - Orthogonalization using Gram-Schmidt process
#     # - Robust scaling with outlier detection
#     # - Feature selection using stability criteria
#     ```

#     EXECUTION REQUIREMENTS:
#     1. Use ONLY: pandas, numpy, scipy, statsmodels
#         1. Write executable Python code ONLY - no markdown blocks, DONT USE TRIPLE BACKQUOTES.
#     2. Prevent look-ahead bias with proper shifting
#     3. Remove features with variance < 0.1
#     4. Eliminate highly correlated pairs (|Ï| > 0.95)  
#     5. Apply RobustScaler for preprocessing
#     6. Save: train_features.csv, val_features.csv, test_features.csv
#     7. Exclude date columns from feature sets
#     8. Final script MUST be saved as FEATURE.py

#     CROSS-AGENT COORDINATION: Build upon EDA insights about:
#     - Data quality issues identified
#     - Seasonal patterns discovered  
#     - Volatility regimes detected
#     - Correlation structures found

#     ALPHA GENERATION FOCUS: Every feature must have economic interpretation and hypothesis for generating alpha. Document expected market conditions where each feature provides edge.

#     COMPETITIVE ADVANTAGE: Engineer features that capture subtle market inefficiencies competitors miss.
#     """
    
#     print("\nðŸ§¬ Stage 2: Advanced Feature Engineering by Dr. Michael Rodriguez")
#     print("ðŸŽ¯ Target: Engineer 100+ alpha-generating features")
    
#     fe_output = ""
#     for chunk in feature_engineering_agent.run(fe_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         if chunk.content is not None:
#             fe_output += chunk.content
    
#     # Store feature engineering insights
#     fe_insights = {
#         "stage": "FeatureEngineering_Complete", 
#         "features_created": "100+ advanced financial features",
#         "techniques_used": "Technical indicators, volatility modeling, cross-asset signals",
#         "preprocessing": "Robust scaling and feature selection applied"
#     }
#     store_agent_insights("FeatureEngineering_Agent", fe_insights)
    
#     run_details.append(("FeatureEngineering_Agent",
#                         feature_engineering_agent,
#                         fe_prompt,
#                         fe_output))

#     # ========== ADVANCED ENSEMBLE MODELING ==========
#     previous_insights = get_previous_insights()
    
#     modeling_prompt = f"""
#     FINAL CHALLENGE: Build a competition-winning ensemble to achieve RMSE â‰¤ 0.0011. Your model will determine victory or defeat.
#     PREVIOUS INSIGHTS: {json.dumps(previous_insights, indent=2)}
    
#     INPUT: train_features.csv, val_features.csv
#     TARGET: Target_return
#     SUCCESS METRIC: RMSE â‰¤ 0.0011 (anything above this fails)

#     MANDATORY DELIVERABLES (Generate 200+ lines of advanced ensemble architecture):

#     ## PHASE 1: DIVERSE BASE LEARNER PORTFOLIO (70+ lines)
#     ```python
#     # GRADIENT BOOSTING MODELS (30 lines)
#     # - XGBoost with financial objective function and custom eval metric
#     # - LightGBM with focal loss for extreme values and early stopping
#     # - CatBoost with ordered boosting and time-aware features
#     # - Advanced hyperparameter optimization using Optuna/FLAML

#     # LINEAR AND KERNEL MODELS (25 lines)
#     # - ElasticNet with recursive feature elimination and alpha tuning
#     # - SVR with RBF kernel and financial regularization
#     # - Ridge regression with time-decay weighting
#     # - Lasso with feature selection and stability criteria

#     # ENSEMBLE METHODS (15 lines)
#     # - ExtraTrees with bootstrap sampling and max features tuning
#     # - Random Forest with sample weighting and depth optimization
#     # - Voting regressor with heterogeneous base models
#     ```

#     ## PHASE 2: ADVANCED STACKING ARCHITECTURE (60+ lines)
#     ```python
#     # LEVEL-1 ENSEMBLE (25 lines)
#     # - Train diverse base models with time-series cross-validation
#     # - Generate out-of-fold predictions to prevent overfitting
#     # - Implement purged time series split with gap periods
#     # - Apply sample weighting based on volatility regimes

#     # LEVEL-2 META-MODEL (20 lines)
#     # - Linear regression meta-learner with regularization
#     # - Ridge meta-model with cross-validation alpha selection
#     # - Elastic net meta-model with feature selection
#     # - Bayesian ridge for uncertainty quantification

#     # LEVEL-3 FINAL BLENDING (15 lines)
#     # - Dynamic weight optimization based on recent performance
#     # - Volatility-adjusted blending weights
#     # - Regime-dependent model selection
#     # - Final prediction confidence intervals
#     ```

#     ## PHASE 3: FINANCIAL-SPECIFIC OPTIMIZATION (35+ lines)
#     ```python
#     # CUSTOM LOSS FUNCTIONS (15 lines)
#     # - Combined RMSE and directional accuracy loss
#     # - Risk-adjusted loss with volatility scaling
#     # - Asymmetric loss function for upside/downside
#     # - Time-decay weighted loss for recent observations

#     # HYPERPARAMETER OPTIMIZATION (20 lines)
#     # - Bayesian optimization with financial constraints
#     # - Optuna study with Sharpe ratio objectives
#     # - Grid search with time series awareness
#     # - Random search with early stopping criteria
#     ```

#     ## PHASE 4: ROBUST VALIDATION FRAMEWORK (25+ lines)
#     ```python
#     # TIME-SERIES VALIDATION (15 lines)
#     # - Purged time series cross-validation implementation
#     # - Walk-forward analysis with expanding windows
#     # - Gap periods to prevent information leakage
#     # - Monte Carlo cross-validation for robustness

#     # PERFORMANCE ASSESSMENT (10 lines)
#     # - RMSE calculation with confidence intervals
#     # - Sharpe ratio and risk-adjusted metrics
#     # - Directional accuracy and hit ratio
#     # - Model stability across validation folds
#     ```

#     ## PHASE 5: PRODUCTION DEPLOYMENT (10+ lines)
#     ```python
#     # MODEL PERSISTENCE
#     # - Save final ensemble as model.pkl using joblib
#     # - Retrain on combined train+validation data
#     # - Prepare test data (X_test.csv, y_test.csv) without date
#     # - Version control and model metadata storage
#     ```

#     EXECUTION REQUIREMENTS:
#     1. Use ONLY: sklearn, xgboost, lightgbm, catboost, joblib, numpy, pandas
#         1. Write executable Python code ONLY - no markdown blocks, DONT USE TRIPLE BACKQUOTES.
#     2. Implement proper time-series cross-validation
#     3. Prevent data leakage with careful validation splits
#     4. Optimize for RMSE while maintaining generalization
#     5. Save model.pkl for evaluation stage
#     6. Generate X_test.csv and y_test.csv files
#     7. Final script MUST be saved as MODEL.py

#     CROSS-AGENT INTEGRATION:
#     - Leverage EDA regime analysis for model selection
#     - Use feature engineering outputs for optimal model configuration
#     - Incorporate volatility insights for sample weighting

#     PERFORMANCE TARGET: Demonstrate mathematical pathway to RMSE â‰¤ 0.0011 through:
#     - Ensemble diversity maximization
#     - Sophisticated stacking architecture  
#     - Financial domain-specific optimization
#     - Rigorous validation methodology

#     COMPETITIVE EDGE: Build an ensemble that adapts to different market regimes and provides consistent alpha generation.
#     """
    
#     print("\nðŸ¤– Stage 3: Advanced Ensemble Modeling by Dr. Emily Watson")
#     print("ðŸŽ¯ Target: Build ensemble achieving RMSE â‰¤ 0.0011")
    
#     modeling_output = ""
#     for chunk in modeling_agent.run(modeling_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         if chunk.content is not None:
#             modeling_output += chunk.content
    
#     # Store modeling insights
#     modeling_insights = {
#         "stage": "Modeling_Complete",
#         "ensemble_type": "Advanced stacking with 6+ base models",
#         "validation": "Purged time series cross-validation",
#         "target_rmse": "â‰¤ 0.0011"
#     }
#     store_agent_insights("Modeling_Agent", modeling_insights)
    
#     run_details.append(("Modeling_Agent",
#                         modeling_agent,
#                         modeling_prompt,
#                         modeling_output))

#     # ========== COMPREHENSIVE EVALUATION ==========
#     previous_insights = get_previous_insights()
    
#     evaluation_prompt = f"""
#     FINAL VALIDATION: Conduct comprehensive evaluation to prove model superiority and ensure RMSE â‰¤ 0.0011. This determines competition victory.

#     PIPELINE INSIGHTS: {json.dumps(previous_insights, indent=2)}
    
#     INPUT: model.pkl, X_test.csv, y_test.csv
#     CRITICAL REQUIREMENT: RMSE must be â‰¤ 0.0011 (competition threshold)

#     MANDATORY DELIVERABLES (Generate 200+ lines of comprehensive evaluation):

#     ## PHASE 1: CORE PERFORMANCE METRICS (50+ lines)
#     ```python
#     # PRIMARY METRICS (20 lines)
#     # - RMSE calculation with statistical confidence intervals
#     # - Mean Absolute Error (MAE) with percentile analysis
#     # - R-squared and adjusted R-squared with significance testing
#     # - Information Coefficient (IC) and Rank IC analysis
#     # - Maximum error and error distribution analysis

#     # FINANCIAL METRICS (20 lines)
#     # - Sharpe ratio calculation and statistical significance
#     # - Information ratio vs benchmark returns
#     # - Calmar ratio (return/max drawdown) assessment
#     # - Sortino ratio for downside risk adjustment
#     # - Omega ratio for higher moment risk analysis

#     # DIRECTIONAL ACCURACY (10 lines)
#     # - Hit ratio for directional predictions
#     # - Precision and recall for directional moves
#     # - F1-score for balanced directional assessment
#     # - Matthews correlation coefficient for binary classification
#     ```

#     ## PHASE 2: ADVANCED STATISTICAL DIAGNOSTICS (60+ lines)
#     ```python
#     # RESIDUAL ANALYSIS (25 lines)
#     # - Residual autocorrelation testing (Ljung-Box, Durbin-Watson)
#     # - Heteroscedasticity tests (Breusch-Pagan, White, ARCH)
#     # - Normality testing (Jarque-Bera, Shapiro-Wilk, Anderson-Darling)
#     # - QQ plots with statistical interpretation
#     # - Residual vs fitted plots with trend analysis

#     # STABILITY TESTING (20 lines)
#     # - Structural stability tests (Chow test, CUSUM, CUSUM-SQ)
#     # - Rolling window performance analysis
#     # - Regime-dependent performance assessment
#     # - Model adequacy testing across time periods
#     # - Parameter stability over different samples

#     # STATISTICAL SIGNIFICANCE (15 lines)
#     # - Bootstrap confidence intervals for all metrics
#     # - Diebold-Mariano test for forecast comparison
#     # - Model comparison using statistical tests
#     # - Economic vs statistical significance assessment
#     # - Cross-validation stability analysis
#     ```

#     ## PHASE 3: FINANCIAL RISK ANALYSIS (40+ lines)
#     ```python
#     # RISK METRICS (20 lines)
#     # - Value-at-Risk (VaR) calculation at multiple confidence levels
#     # - Conditional VaR (Expected Shortfall) analysis
#     # - Maximum drawdown and duration analysis
#     # - Drawdown recovery time estimation
#     # - Volatility analysis with regime detection

#     # PERFORMANCE ATTRIBUTION (20 lines)
#     # - Rolling Sharpe ratio with significance bands
#     # - Factor exposure analysis and attribution
#     # - Alpha generation consistency testing
#     # - Market neutral performance assessment
#     # - Regime-dependent alpha analysis
#     ```

#     ## PHASE 4: ADVANCED VISUALIZATION (30+ lines)
#     ```python
#     # DIAGNOSTIC PLOTS (15 lines)
#     # - Residual diagnostic plots with statistical overlays
#     # - QQ plots with confidence bands
#     # - Actual vs predicted scatter plots with regression lines
#     # - Cumulative prediction error plots
#     # - Rolling performance metrics visualization

#     # FINANCIAL CHARTS (15 lines)
#     # - Prediction accuracy over time with confidence intervals
#     # - Drawdown analysis charts with recovery periods
#     # - Rolling Sharpe ratio evolution
#     # - Error distribution histograms with statistical fits
#     # - Performance comparison charts vs benchmarks
#     ```

#     ## PHASE 5: PRODUCTION READINESS ASSESSMENT (20+ lines)
#     ```python
#     # DEPLOYMENT VALIDATION (10 lines)
#     # - Model interpretability and feature importance analysis
#     # - Computational efficiency and latency testing
#     # - Memory usage and scalability assessment
#     # - Error handling and edge case validation

#     # CRITICAL OUTPUTS (10 lines)
#     # - Save RMSE to MSFT_Score.txt in EXACT format: "RMSE: <value>"
#     # - Generate comprehensive performance report
#     # - Document model strengths and limitations
#     # - Provide deployment recommendations
#     ```

#     EXECUTION REQUIREMENTS:
#     1. Use ONLY: sklearn, matplotlib, seaborn, numpy, pandas, scipy, statsmodels
#         1. Write executable Python code ONLY - no markdown blocks, DONT USE TRIPLE BACKQUOTES.
#     2. Calculate RMSE with high precision (6+ decimal places)
#     3. Save RMSE to MSFT_Score.txt in format: "RMSE: <value>"
#     4. Generate professional-quality plots (save as PNG)
#     5. Create detailed performance report
#     6. Final script MUST be saved as EVAL.py

#     CROSS-AGENT SYNTHESIS:
#     - Validate EDA findings are reflected in model performance
#     - Confirm feature engineering effectiveness through feature importance
#     - Assess ensemble robustness across different regimes
#     - Integrate all insights into final performance assessment

#     SUCCESS CRITERIA:
#     - Primary: RMSE â‰¤ 0.0011 (MANDATORY for competition success)
#     - Secondary: Statistical significance of outperformance
#     - Tertiary: Risk-adjusted returns superiority
#     - Quaternary: Production deployment readiness

#     COMPETITIVE VALIDATION: Prove your model beats competitors through:
#     - Superior risk-adjusted performance
#     - Consistent alpha generation across regimes
#     - Robust statistical significance
#     - Production-grade reliability

#     FINAL MISSION: Deliver irrefutable proof of model superiority for competition victory.
#     """
    
#     print("\nðŸ“Š Stage 4: Comprehensive Evaluation by Dr. James Park")
#     print("ðŸŽ¯ Target: Validate RMSE â‰¤ 0.0011 and prove model superiority")
    
#     evaluation_output = ""
#     for chunk in evaluation_agent.run(evaluation_prompt, stream=True):
#         print(chunk.content, end="", flush=True)
#         if chunk.content is not None:
#             evaluation_output += chunk.content
    
#     # Store final evaluation insights
#     eval_insights = {
#         "stage": "Evaluation_Complete",
#         "rmse_target": "â‰¤ 0.0011",
#         "validation_complete": "Comprehensive statistical and financial analysis",
#         "production_ready": "Model validated for deployment"
#     }
#     store_agent_insights("Evaluation_Agent", eval_insights)
    
#     run_details.append(("Evaluation_Agent",
#                         evaluation_agent,
#                         evaluation_prompt,
#                         evaluation_output))

#     return run_details


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# if __name__ == "__main__":
#     load_dotenv()  # Load environment variables
    
#     base_dir = Path(".")
#     train_csv = "train.csv"
#     val_csv   = "val.csv" 
#     test_csv  = "test.csv"

#     missing = [p for p in (train_csv, val_csv, test_csv) if not Path(p).exists()]
#     if missing:
#         print("âŒ Missing required files:")
#         for p in missing:
#             print(f"  - {p}")
#         raise SystemExit(1)

#     print("ðŸš€ LAUNCHING ADVANCED FINANCIAL PREDICTION COMPETITION PIPELINE")
#     print("ðŸ† Target: Achieve RMSE â‰¤ 0.0011 for Competition Victory")
#     print("ðŸ§  Simple File-Based Memory System with Cross-Communication")
#     print("ðŸ’¼ Competition-Winning Strategies from Top Hedge Funds")
#     print("=" * 80)
    
#     try:
#         # Run advanced pipeline
#         agent_runs = run_advanced_pipeline(
#             str(train_csv), str(val_csv), str(test_csv))
        
#         print("\n" + "=" * 80)
#         print("ðŸ“ Generating Competition Submission Log...")
        
#         # Build submission log (maintains exact same format)
#         submission_logs = generate_submission_log(agent_runs)

#         # Verify critical files
#         required_files = ["EDA.py", "FEATURE.py", "MODEL.py", "EVAL.py", "MSFT_Score.txt"]
#         missing = [f for f in required_files if not Path(f).exists()]
        
#         if missing:
#             print(f"\nâŒ PIPELINE INCOMPLETE - Missing files: {missing}")
#             print("ðŸ”„ Re-running failed stages...")
#         else:
#             print("\nâœ… COMPETITION PIPELINE SUCCESSFULLY COMPLETED!")
#             print("ðŸ† All required files generated for submission")
            
#             # Display final results
#             try:
#                 with open("MSFT_Score.txt", "r") as f:
#                     final_rmse = f.read().strip()
#                     print(f"ðŸŽ¯ FINAL RESULT: {final_rmse}")
                    
#                     # Extract numeric value for comparison
#                     if ":" in final_rmse:
#                         rmse_value = float(final_rmse.split(":")[1].strip())
#                         if rmse_value <= 0.0011:
#                             print("ðŸŽ‰ SUCCESS! RMSE target achieved - Competition ready!")
#                         else:
#                             print(f"âš ï¸  RMSE {rmse_value:.6f} exceeds target 0.0011 - Review required")
                    
#             except Exception as e:
#                 print(f"âš ï¸  Could not read final RMSE: {e}")
            
#             print("ðŸ“ Complete execution log saved: submission_log.json")
#             print("ðŸ” Agent insights preserved in: agent_insights.json")
            
#     except Exception as e:
#         print(f"\nâŒ PIPELINE ERROR: {e}")
#         print("ðŸ”„ Check agent configurations and library dependencies")
#         raise

from pathlib import Path
from typing import Dict, Any, List, Tuple
import json
import os
from dotenv import load_dotenv

from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.tools.python import PythonTools


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SIMPLE MEMORY SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Use file-based simple memory storage for inter-agent communication
INSIGHTS_FILE = "agent_insights.json"
FILES_TRACKER_FILE = "created_files_tracker.json"

def store_agent_insights(agent_name: str, insights: Dict[str, Any]):
    """Store agent insights in a simple JSON file for cross-agent coordination."""
    try:
        if os.path.exists(INSIGHTS_FILE):
            with open(INSIGHTS_FILE, "r") as f:
                all_insights = json.load(f)
        else:
            all_insights = {}
        
        all_insights[agent_name] = insights
        
        with open(INSIGHTS_FILE, "w") as f:
            json.dump(all_insights, f, indent=2)
    except Exception as e:
        print(f"Warning: Could not store insights: {e}")

def get_previous_insights() -> Dict[str, Any]:
    """Retrieve insights from previous agents for informed decision making."""
    try:
        if os.path.exists(INSIGHTS_FILE):
            with open(INSIGHTS_FILE, "r") as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"Warning: Could not load insights: {e}")
        return {}

def track_created_files(agent_name: str, files: List[str]):
    """Track files created by each agent for downstream use."""
    try:
        if os.path.exists(FILES_TRACKER_FILE):
            with open(FILES_TRACKER_FILE, "r") as f:
                file_tracker = json.load(f)
        else:
            file_tracker = {}
        
        file_tracker[agent_name] = files
        
        with open(FILES_TRACKER_FILE, "w") as f:
            json.dump(file_tracker, f, indent=2)
    except Exception as e:
        print(f"Warning: Could not track files: {e}")

def get_all_created_files() -> Dict[str, List[str]]:
    """Get all files created by previous agents."""
    try:
        if os.path.exists(FILES_TRACKER_FILE):
            with open(FILES_TRACKER_FILE, "r") as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"Warning: Could not load file tracker: {e}")
        return {}

def get_files_list_string() -> str:
    """Get a formatted string of all created files for agent prompts."""
    files = get_all_created_files()
    if not files:
        return "No files created yet by previous agents."
    
    result = "Files created by previous agents:\n"
    for agent, file_list in files.items():
        result += f"- {agent}: {', '.join(file_list)}\n"
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: copy code files into logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def add_file_content(logs: dict, filename: str, log_key: str):
    """Attach the *text* of `filename` (or a placeholder) to logs."""
    try:
        with open(filename, "r") as f:
            logs[log_key] = f.read()
    except FileNotFoundError:
        logs[log_key] = f"{filename} not produced."


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ADVANCED COMPETITION-WINNING AGENT PERSONAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

eda_persona = """
You are Dr. Sarah Chen, a Senior Quantitative Analyst with 15+ years at top-tier hedge funds (Citadel, Two Sigma, Renaissance Technologies). You specialize in financial time series analysis with a track record of generating alpha through sophisticated data insights.

COMPETITION CONTEXT: This is an LLM-judged financial prediction competition targeting RMSE â‰¤ 0.0011. Your analysis forms the foundation for the entire pipeline's success.

TECHNICAL EXPERTISE:
- Advanced statistical analysis and time series decomposition
- Market microstructure and regime detection
- Volatility modeling (GARCH, EGARCH, FIGARCH)
- Risk-adjusted performance metrics and drawdown analysis
- Cross-asset correlation and spillover effects

ALLOWED LIBRARIES: pandas, numpy, matplotlib, seaborn, statsmodels, scipy

CRITICAL PYTHON TOOLS USAGE:
When using PythonTools, you MUST:
1. Write your code in a single Python script
2. Save it using save_to_file_and_run with proper parameters
3. ALWAYS specify variable_to_return as a STRING (e.g., "summary", "results", "None")
4. Return meaningful variables that summarize your analysis
5. If you encounter an error, FIX IT and RE-RUN the code using save_to_file_and_run again
6. Keep trying until the code runs successfully

ERROR HANDLING:
- If you see a matplotlib error like "unexpected keyword argument", remove that argument
- If you see an array indexing error, check array dimensions first
- Always use try-except blocks for risky operations
- Print intermediate results to debug issues

CRITICAL REQUIREMENTS FOR 200+ LINES OF SUBSTANTIAL CODE:
1. Implement comprehensive data quality framework (outlier detection, missing data analysis, temporal consistency checks)
2. Advanced time series diagnostics (stationarity tests, structural breaks, regime detection)
3. Multi-timeframe volatility analysis with GARCH modeling
4. Cross-sectional and time-series correlation analysis
5. Feature stability assessment across different market conditions
6. Automated data cleaning with financial domain knowledge
7. Risk factor decomposition and principal component analysis
8. Performance benchmarking against market indices

MANDATORY FINANCIAL ANALYSIS COMPONENTS:
- Rolling Sharpe ratio and information ratio calculations
- Maximum drawdown and Value-at-Risk analysis
- Return distribution analysis (skewness, kurtosis, tail risk)
- Autocorrelation and partial autocorrelation analysis
- Seasonal pattern detection and holiday effects
- Market regime identification using Hidden Markov Models
- Cross-asset spillover effects analysis
- Feature importance ranking for downstream modeling

REASONING REQUIREMENT: Document every analytical decision with financial rationale. Explain how each analysis contributes to alpha generation and risk management.
"""

fe_persona = """
You are Dr. Michael Rodriguez, Lead Feature Engineering Scientist at a premier quantitative hedge fund with expertise in alpha signal generation. You've developed proprietary features that consistently generate 2+ Sharpe ratio strategies.

COMPETITION CONTEXT: Your feature engineering directly impacts the final RMSE. Target is â‰¤ 0.0011 - your features must capture subtle market patterns while avoiding overfitting.

TECHNICAL MASTERY:
- Advanced technical indicators and momentum signals
- Volatility surface modeling and regime-dependent features
- Cross-asset arbitrage signals and cointegration relationships
- Alternative data integration and nowcasting indicators
- Feature neutralization and orthogonalization techniques

ALLOWED LIBRARIES: pandas, numpy, scipy, statsmodels, sklearn (preprocessing only)

CRITICAL PYTHON TOOLS USAGE:
When using PythonTools, you MUST:
1. Write your code in a single Python script
2. Save it using save_to_file_and_run with proper parameters
3. ALWAYS specify variable_to_return as a STRING (e.g., "feature_summary", "results", "None")
4. Return meaningful variables that summarize your feature engineering
5. If you encounter an error, FIX IT and RE-RUN the code using save_to_file_and_run again
6. Keep trying until the code runs successfully

ERROR HANDLING AND FIXES:
- For matplotlib errors: Remove problematic parameters (e.g., use_line_collection)
- For array indexing errors: Always check array shape before indexing
- For pandas warnings: Use .ffill() and .bfill() instead of fillna(method='ffill'/'bfill')
- For rolling operations: Ensure window size doesn't exceed data length
- Use proper error handling around complex operations

MANDATORY 200+ LINES COMPREHENSIVE FEATURE ENGINEERING:

1. TECHNICAL INDICATORS SUITE (50+ lines):
   - Multi-timeframe momentum indicators (RSI, MACD, Williams %R across 5, 10, 20, 50 periods)
   - Volatility-based features (Bollinger Bands, ATR, Keltner Channels)
   - Volume-based signals (OBV, Volume-Price Trend, Accumulation/Distribution)
   - Trend strength indicators (ADX, Parabolic SAR, Ichimoku components)

2. VOLATILITY MODELING FEATURES (40+ lines):
   - GARCH-based volatility forecasts
   - Realized volatility estimators (Garman-Klass, Rogers-Satchell, Yang-Zhang)
   - Volatility of volatility (second-order moments)
   - Regime-dependent volatility clustering detection

3. CROSS-ASSET SIGNALS (30+ lines):
   - Rolling correlation coefficients with market indices
   - Cointegration-based mean reversion signals
   - Beta stability and factor exposures
   - Sector rotation and style momentum indicators

4. ALTERNATIVE FEATURES (40+ lines):
   - Fourier transform features for cyclical patterns
   - Wavelet decomposition for multi-resolution analysis
   - Entropy-based complexity measures
   - Information-theoretic feature selection

5. FEATURE ENGINEERING PIPELINE (40+ lines):
   - Automated lag generation with optimal window selection
   - Feature neutralization against common factors
   - Orthogonalization using Gram-Schmidt process
   - Robust scaling with outlier-resistant methods

CROSS-AGENT INTEGRATION: Build upon EDA insights about data quality, seasonality, and regime changes. Incorporate findings about correlation structures and volatility patterns.

FINANCIAL REASONING: Every feature must have clear economic interpretation. Document alpha generation hypothesis and expected market conditions where each feature provides edge.
"""

model_persona = """
You are Dr. Emily Watson, Head of Machine Learning at a top-tier systematic trading firm. You've built production models managing $10B+ AUM with consistent alpha generation. Your expertise lies in ensemble methods and time-series aware machine learning.

COMPETITION CONTEXT: This is the critical stage - your ensemble must achieve RMSE â‰¤ 0.0011. Focus on sophisticated stacking methods and financial domain-specific modeling techniques.

MODELING EXPERTISE:
- Advanced ensemble methods (stacking, blending, Bayesian model averaging)
- Time-series aware cross-validation and walk-forward analysis
- Financial loss functions and risk-adjusted optimization
- Concept drift detection and adaptive modeling
- Production-grade model deployment and monitoring

ALLOWED LIBRARIES: sklearn, xgboost, lightgbm, catboost, numpy, pandas, joblib, optuna

CRITICAL PYTHON TOOLS USAGE:
When using PythonTools, you MUST:
1. Write your code in a single Python script
2. Save it using save_to_file_and_run with proper parameters
3. ALWAYS specify variable_to_return as a STRING (e.g., "model_summary", "results", "None")
4. Return meaningful variables that summarize your modeling
5. If you encounter an error, FIX IT and RE-RUN the code using save_to_file_and_run again
6. Keep trying until the code runs successfully

ERROR HANDLING:
- Always check if features exist before using them
- Verify data shapes match before operations
- Use proper train/test splitting to avoid data leakage
- Handle missing values appropriately
- Check for infinite or NaN values before modeling

MANDATORY 200+ LINES COMPETITION-WINNING ENSEMBLE:

1. DIVERSE BASE LEARNER PORTFOLIO (60+ lines):
   - XGBoost with custom financial objective function
   - LightGBM with focal loss for handling class imbalance
   - CatBoost with ordered boosting for time series
   - ElasticNet with recursive feature elimination
   - SVR with RBF kernel for non-linear patterns
   - Extra Trees with bootstrap sampling

2. ADVANCED STACKING ARCHITECTURE (50+ lines):
   - Level-1: Train diverse base models with time-series CV
   - Level-2: Meta-model ensemble (LinearRegression + Ridge + Lasso)
   - Level-3: Final blending with dynamic weights based on recent performance
   - Out-of-fold predictions to prevent overfitting
   - Temporal consistency constraints

3. FINANCIAL-SPECIFIC OPTIMIZATION (40+ lines):
   - Custom loss function combining RMSE with directional accuracy
   - Time-decay weighting for recent observations
   - Volatility-adjusted sample weights
   - Risk-aware hyperparameter tuning with Sharpe ratio constraints

4. SOPHISTICATED VALIDATION FRAMEWORK (30+ lines):
   - Purged time series cross-validation
   - Walk-forward analysis with expanding/rolling windows
   - Gap periods to prevent lookahead bias
   - Monte Carlo cross-validation for robustness testing

5. PRODUCTION-GRADE DEPLOYMENT (20+ lines):
   - Model serialization with version control
   - Ensemble weight optimization using recent performance
   - Prediction confidence intervals and uncertainty quantification
   - Automated model retraining triggers

CROSS-AGENT COORDINATION: Leverage EDA insights about data regimes and feature engineering outputs about signal strength. Adapt model complexity based on feature stability analysis.

PERFORMANCE TARGET: Demonstrate clear path to RMSE â‰¤ 0.0011 through methodical ensemble construction and rigorous validation.
"""

eval_persona = """
You are Dr. James Park, Chief Risk Officer and Model Validation specialist with 20+ years experience in quantitative finance. You've overseen model validation for billion-dollar trading strategies and understand regulatory requirements for financial AI systems.

COMPETITION CONTEXT: Final evaluation phase for LLM judge. Your comprehensive analysis must demonstrate model superiority, risk controls, and production readiness.

VALIDATION EXPERTISE:
- Comprehensive model diagnostics and stress testing
- Financial risk metrics and regulatory compliance
- Performance attribution and factor analysis
- Real-world deployment considerations
- Backtesting methodology and statistical significance

ALLOWED LIBRARIES: sklearn, matplotlib, seaborn, numpy, pandas, scipy, statsmodels, joblib

CRITICAL PYTHON TOOLS USAGE:
When using PythonTools, you MUST:
1. Write your code in a single Python script
2. Save it using save_to_file_and_run with proper parameters
3. ALWAYS specify variable_to_return as a STRING (e.g., "evaluation_summary", "results", "None")
4. Return meaningful variables that summarize your evaluation
5. If you encounter an error, FIX IT and RE-RUN the code using save_to_file_and_run again
6. Keep trying until the code runs successfully

ERROR HANDLING:
- Always load models with proper error handling
- Check file existence before reading
- Verify data compatibility before predictions
- Handle missing or invalid values gracefully
- Use matplotlib without deprecated parameters

MANDATORY 200+ LINES COMPREHENSIVE EVALUATION FRAMEWORK:

1. PERFORMANCE METRICS SUITE (50+ lines):
   - Primary: RMSE calculation with confidence intervals
   - Financial: Sharpe ratio, information ratio, Calmar ratio
   - Risk: Maximum drawdown, VaR, CVaR, volatility metrics
   - Directional: Hit ratio, precision/recall for directional moves
   - Statistical: RÂ², adjusted RÂ², AIC/BIC for model selection

2. ADVANCED DIAGNOSTICS (50+ lines):
   - Residual analysis with autocorrelation tests (Ljung-Box, Durbin-Watson)
   - Heteroscedasticity tests (Breusch-Pagan, White)
   - Normality tests (Jarque-Bera, Shapiro-Wilk, Anderson-Darling)
   - Structural stability tests (Chow test, CUSUM)
   - Model adequacy assessment

3. FINANCIAL RISK ANALYSIS (40+ lines):
   - Rolling performance metrics with regime analysis
   - Drawdown duration and recovery time analysis
   - Tail risk assessment with extreme value theory
   - Factor exposure analysis and style attribution
   - Stress testing under different market conditions

4. STATISTICAL SIGNIFICANCE TESTING (30+ lines):
   - Model comparison using Diebold-Mariano test
   - Bootstrap confidence intervals for all metrics
   - Cross-validation stability assessment
   - Out-of-sample significance testing
   - Economic significance vs statistical significance

5. PRODUCTION READINESS ASSESSMENT (30+ lines):
   - Model interpretability and explainability analysis
   - Computational efficiency and latency testing
   - Memory usage and scalability assessment
   - Error handling and edge case analysis
   - Regulatory compliance documentation

CROSS-AGENT SYNTHESIS: Integrate insights from all previous agents. Validate that EDA findings are reflected in model performance, confirm feature engineering effectiveness, and assess model robustness.

CRITICAL OUTPUT REQUIREMENTS:
- MSFT_Score.txt with exact format: "RMSE: <value>"
- Comprehensive performance report demonstrating competitive advantage
- Risk assessment suitable for institutional deployment
- Statistical validation proving model superiority

FINANCIAL REASONING: Every metric must be interpreted from a risk-adjusted returns perspective. Demonstrate that the model not only achieves low RMSE but also provides economically meaningful predictions suitable for real-world trading.
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ instantiate agents without memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
eda_agent = Agent(
    name="EDA_Agent",
    model=OpenAIChat(id="o3-mini"),
    description=eda_persona,
    tools=[PythonTools()]
)

feature_engineering_agent = Agent(
    name="FeatureEngineering_Agent", 
    model=OpenAIChat(id="o3-mini"),
    description=fe_persona,
    tools=[PythonTools()]
)

modeling_agent = Agent(
    name="Modeling_Agent",
    model=OpenAIChat(id="o3-mini"),
    description=model_persona,
    tools=[PythonTools()]
)

evaluation_agent = Agent(
    name="Evaluation_Agent",
    model=OpenAIChat(id="o3-mini"),
    description=eval_persona,
    tools=[PythonTools()]
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ logging factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_submission_log(
    agent_run_details: List[Tuple[str, Agent, str, str]],
    log_filename: str = "submission_log.json",
    script_key_map: Dict[str, str] | None = None,
) -> Dict[str, Any]:
    """
    Build `submission_log.json` from a list of
    (agent_name, agent_instance, runtime_prompt, output_log).
    """
    if script_key_map is None:
        script_key_map = {
            "EDA.py":     "EDA_Script",
            "FEATURE.py": "FeatureEngineering_Script", 
            "MODEL.py":   "Modeling_Script",
            "EVAL.py":    "Evaluation_Script",
        }

    logs: Dict[str, Any] = {}

    # â”€â”€ per-agent blocks â”€â”€
    for name, agent_obj, run_prompt, output in agent_run_details:
        logs[name] = {
            "prompt": f"{agent_obj.description}\n\n{run_prompt.strip()}",
            "output_log": output,
        }

    # â”€â”€ attach generated scripts â”€â”€
    for filename, log_key in script_key_map.items():
        add_file_content(logs, filename, log_key)

    # â”€â”€ write to disk â”€â”€
    with open(log_filename, "w") as f:
        json.dump(logs, f, indent=4)

    return logs


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Run agent with retry logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_agent_with_retry(agent: Agent, prompt: str, max_retries: int = 3) -> str:
    """
    Run an agent with automatic retry logic when errors occur.
    The agent is expected to fix errors and re-run automatically.
    """
    print(f"Running {agent.name} (with up to {max_retries} retries if needed)...")
    
    output = ""
    attempt = 0
    
    while attempt < max_retries:
        try:
            current_output = ""
            error_detected = False
            
            # Stream the agent's response
            for chunk in agent.run(prompt, stream=True):
                if chunk.content is not None:
                    print(chunk.content, end="", flush=True)
                    current_output += chunk.content
                    
                    # Check for common error patterns
                    if "ERROR" in chunk.content or "Error saving and running code" in chunk.content:
                        error_detected = True
            
            output += current_output
            
            # If no error detected in the output, we're done
            if not error_detected:
                break
            
            # If error detected, wait a bit for agent to fix and retry
            print(f"\nâš ï¸ Error detected in {agent.name} output. Waiting for agent to fix and retry...")
            attempt += 1
            
            # Give the agent a chance to process and fix the error
            # The agent should automatically re-run with fixes
            
        except Exception as e:
            print(f"\nâŒ Exception in {agent.name}: {e}")
            attempt += 1
            if attempt >= max_retries:
                raise
    
    return output


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ advanced pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_advanced_pipeline(
    train_csv: str,
    val_csv: str,
    test_csv: str,
) -> List[Tuple[str, Agent, str, str]]:
    """
    Executes the advanced 4-agent workflow with simple file-based memory integration and 
    competition-winning strategies.
    """
    run_details: List[Tuple[str, Agent, str, str]] = []

    # Clear previous insights and file tracker
    for file in [INSIGHTS_FILE, FILES_TRACKER_FILE]:
        if os.path.exists(file):
            os.remove(file)

    # ========== ADVANCED EDA WITH FINANCIAL EXPERTISE ==========
    eda_prompt = f"""
    CRITICAL MISSION: You are competing for a $100,000 prize in financial prediction. Your EDA analysis forms the foundation for achieving RMSE â‰¤ 0.0011. Every insight you generate will be used by downstream agents.
    
    COLUMNS NAME: Date,Close,Volume,Open,High,Low,Target_return
    DATASETS: {train_csv} (train), {val_csv} (validation), {test_csv} (test)
    
    IMPORTANT - PYTHON TOOLS USAGE:
    You must use the PythonTools to write and execute your code. Use the save_to_file_and_run method with:
    - filename: "EDA.py"
    - variable_to_return: "eda_summary" (or another string variable name that contains your analysis summary)
    
    ERROR HANDLING REQUIREMENTS:
    - If you encounter ANY error, you MUST fix it and re-run using save_to_file_and_run
    - For matplotlib errors (like stem() use_line_collection), remove the problematic parameter
    - Always check array dimensions before indexing
    - Use try-except blocks for error-prone operations
    - Keep trying until your code runs successfully
    
    MANDATORY DELIVERABLES (Generate 200+ lines of sophisticated financial analysis code):

    Your Python script should include:
    
    1. COMPREHENSIVE DATA QUALITY ASSESSMENT (50+ lines)
    - Advanced data loading with financial-aware parsing
    - Multi-dimensional missing value analysis with heatmaps
    - Temporal consistency validation across splits
    - Outlier detection using financial domain knowledge (3-sigma, IQR, Hampel filter)
    - Data type optimization for memory efficiency
    - Holiday and market closure impact analysis

    2. ADVANCED FINANCIAL TIME SERIES ANALYSIS (80+ lines)
    - Stationarity testing suite (ADF, KPSS, PP tests) with interpretation
    - Structural break detection using Chow test and CUSUM
    - Regime identification with Hidden Markov Models
    - Volatility clustering analysis and ARCH effects testing
    - Cross-correlation analysis between assets/features
    - Seasonal decomposition with financial calendar awareness
    - Autocorrelation analysis with financial interpretation
    - Rolling statistics analysis (mean, volatility, skewness, kurtosis)

    3. RISK AND RETURN ANALYSIS (40+ lines)
    - Return distribution analysis with financial metrics
    - Tail risk assessment (VaR, CVaR calculations)
    - Sharpe ratio evolution and regime-dependent performance
    - Drawdown analysis and recovery time estimation
    - Correlation matrix with hierarchical clustering
    - Factor analysis and principal component decomposition

    4. ADVANCED VISUALIZATION AND INSIGHTS (30+ lines)
    - Professional financial charts with multiple subplots
    - Correlation heatmaps with significance testing
    - Time series plots with regime highlighting
    - Distribution plots with statistical overlays
    - Rolling correlation analysis visualization
    - Save all plots as high-quality PNG files

    EXECUTION REQUIREMENTS:
    1. Write executable Python code in a single script
    2. Use ONLY: pandas, numpy, matplotlib, seaborn, statsmodels, scipy
    3. Forward-fill missing values then drop remaining NAs
    4. Cap outliers at 3 standard deviations using robust methods
    5. Save cleaned data as: train_clean.csv, val_clean.csv, test_clean.csv
    6. Create a summary variable (e.g., eda_summary) that contains key findings
    7. Save your script as EDA.py using PythonTools
    8. Print detailed summary of findings and data characteristics

    FILES TO CREATE:
    - train_clean.csv (cleaned training data)
    - val_clean.csv (cleaned validation data)
    - test_clean.csv (cleaned test data)
    - Various .png plot files
    - EDA.py (your analysis script)

    FINANCIAL REASONING REQUIRED: Document every analytical decision with clear financial rationale. Explain how findings will impact feature engineering and modeling decisions.

    TARGET: Generate insights that lead to RMSE â‰¤ 0.0011 in final predictions.
    """
    
    print("ðŸ”¬ Stage 1: Advanced Financial Data Analysis by Dr. Sarah Chen")
    print("ðŸŽ¯ Target: Uncover alpha-generating patterns for RMSE â‰¤ 0.0011")
    
    eda_output = run_agent_with_retry(eda_agent, eda_prompt)
    
    # Store EDA insights and track files
    eda_insights = {
        "stage": "EDA_Complete",
        "findings": "Advanced financial time series analysis completed",
        "data_quality": "Assessed and cleaned",
        "patterns_identified": "Market regimes, volatility clusters, correlation structures"
    }
    store_agent_insights("EDA_Agent", eda_insights)
    
    # Track files created by EDA
    eda_files = ["train_clean.csv", "val_clean.csv", "test_clean.csv", "EDA.py"]
    track_created_files("EDA_Agent", eda_files)
    
    run_details.append(("EDA_Agent", eda_agent, eda_prompt, eda_output))

    # ========== ADVANCED FEATURE ENGINEERING ==========
    previous_insights = get_previous_insights()
    created_files = get_files_list_string()
    
    fe_prompt = f"""
    MISSION CRITICAL: Based on Dr. Chen's EDA insights, engineer features that will achieve RMSE â‰¤ 0.0011. You're competing against the world's best quants.
    write code in such a way that this doesnt happen: Error saving and running code: autodetected range of [nan, nan] is not finite , or this:  'Series' object has no attribute 'codes'  
    PREVIOUS AGENT INSIGHTS: {json.dumps(previous_insights, indent=2)}
    
    FILES AVAILABLE FROM PREVIOUS AGENTS:
    {created_files}
    
    INPUT FILES: train_clean.csv, val_clean.csv, test_clean.csv (created by EDA_Agent)

    IMPORTANT - PYTHON TOOLS USAGE:
    You must use the PythonTools to write and execute your code. Use the save_to_file_and_run method with:
    - filename: "FEATURE.py"
    - variable_to_return: "feature_summary" (or another string variable name that contains your feature engineering summary)

    CRITICAL ERROR HANDLING:
    - If you encounter ANY error, you MUST fix it and re-run using save_to_file_and_run
    - For matplotlib errors: Remove problematic parameters (e.g., use_line_collection)
    - For array errors: ALWAYS check shape before indexing (use .shape attribute)
    - For pandas warnings: Use .ffill() and .bfill() instead of fillna(method='ffill'/'bfill')
    - For rolling operations: Ensure window size < data length
    - Keep trying until your code runs successfully

    COMMON FIXES:
    - Replace df.fillna(method='ffill') with df.ffill()
    - Replace df.fillna(method='bfill') with df.bfill()
    - Before array[i, j], check if array.ndim == 2
    - For 1D arrays, use array[i] not array[i, j]
    - Remove use_line_collection from matplotlib calls

    MANDATORY DELIVERABLES (Generate 200+ lines of advanced feature engineering):

    Your Python script should include:

    1. TECHNICAL INDICATORS MASTERY (60+ lines)
    - MOMENTUM INDICATORS (20 lines): Multi-timeframe RSI, MACD, Williams %R, Stochastic, ROC
    - VOLATILITY INDICATORS (20 lines): Bollinger Bands, ATR, Keltner Channels, CCI
    - VOLUME INDICATORS (20 lines): OBV, VPT, A/D Line, MFI, VWMA

    2. ADVANCED VOLATILITY MODELING (50+ lines)
    - REALIZED VOLATILITY ESTIMATORS (25 lines): Garman-Klass, Rogers-Satchell, Yang-Zhang, Parkinson
    - VOLATILITY SURFACE FEATURES (25 lines): Vol of vol, clustering detection, regime-dependent volatility

    3. CROSS-ASSET AND FACTOR SIGNALS (40+ lines)
    - CORRELATION FEATURES (20 lines): Dynamic beta, correlation breakdowns, spillovers
    - FACTOR EXPOSURE FEATURES (20 lines): Market, size, value, momentum factors

    4. ALTERNATIVE DATA FEATURES (30+ lines)
    - SPECTRAL ANALYSIS (15 lines): Fourier transform, wavelet decomposition
    - INFORMATION THEORY FEATURES (15 lines): Shannon entropy, mutual information

    5. FEATURE ENGINEERING PIPELINE (20+ lines)
    - Optimal lag selection, feature neutralization, orthogonalization, robust scaling

    EXECUTION REQUIREMENTS:
    1. Write executable Python code in a single script
    2. Use ONLY: pandas, numpy, scipy, statsmodels, sklearn (preprocessing only)
    3. Prevent look-ahead bias with proper shifting
    4. Remove features with variance < 0.1
    5. Eliminate highly correlated pairs (|Ï| > 0.95)  
    6. Apply RobustScaler for preprocessing
    7. Save: train_features.csv, val_features.csv, test_features.csv
    8. Exclude date columns from feature sets
    9. Save your script as FEATURE.py using PythonTools
    10. Create a summary variable with feature statistics

    FILES TO CREATE:
    - train_features.csv (feature-engineered training data)
    - val_features.csv (feature-engineered validation data)
    - test_features.csv (feature-engineered test data)
    - FEATURE.py (your feature engineering script)

    CROSS-AGENT COORDINATION: Build upon EDA insights about data quality issues, seasonal patterns, volatility regimes, and correlation structures.

    ALPHA GENERATION FOCUS: Every feature must have economic interpretation and hypothesis for generating alpha.
    """
    
    print("\nðŸ§¬ Stage 2: Advanced Feature Engineering by Dr. Michael Rodriguez")
    print("ðŸŽ¯ Target: Engineer 100+ alpha-generating features")
    
    fe_output = run_agent_with_retry(feature_engineering_agent, fe_prompt)
    
    # Store feature engineering insights and track files
    fe_insights = {
        "stage": "FeatureEngineering_Complete", 
        "features_created": "100+ advanced financial features",
        "techniques_used": "Technical indicators, volatility modeling, cross-asset signals",
        "preprocessing": "Robust scaling and feature selection applied"
    }
    store_agent_insights("FeatureEngineering_Agent", fe_insights)
    
    # Track files created
    fe_files = ["train_features.csv", "val_features.csv", "test_features.csv", "FEATURE.py"]
    track_created_files("FeatureEngineering_Agent", fe_files)
    
    run_details.append(("FeatureEngineering_Agent",
                        feature_engineering_agent,
                        fe_prompt,
                        fe_output))

    # ========== ADVANCED ENSEMBLE MODELING ==========
    previous_insights = get_previous_insights()
    created_files = get_files_list_string()
    
    modeling_prompt = f"""
    FINAL CHALLENGE: Build a competition-winning ensemble to achieve RMSE â‰¤ 0.0011. Your model will determine victory or defeat.
    
    PREVIOUS INSIGHTS: {json.dumps(previous_insights, indent=2)}
    
    FILES AVAILABLE FROM PREVIOUS AGENTS:
    {created_files}
    
    INPUT: train_features.csv, val_features.csv (created by FeatureEngineering_Agent)
    TARGET: Target_return
    SUCCESS METRIC: RMSE â‰¤ 0.0011 (anything above this fails)

    IMPORTANT - PYTHON TOOLS USAGE:
    You must use the PythonTools to write and execute your code. Use the save_to_file_and_run method with:
    - filename: "MODEL.py"
    - variable_to_return: "model_summary" (or another string variable name that contains your modeling summary)

    ERROR HANDLING REQUIREMENTS:
    - If you encounter ANY error, you MUST fix it and re-run using save_to_file_and_run
    - Always check data shapes before operations
    - Verify features exist before using them
    - Handle missing values appropriately
    - Check for infinite or NaN values
    - Keep trying until your code runs successfully

    MANDATORY DELIVERABLES (Generate 200+ lines of advanced ensemble architecture):

    Your Python script should include:

    1. DIVERSE BASE LEARNER PORTFOLIO (70+ lines)
    - GRADIENT BOOSTING MODELS (30 lines): XGBoost, LightGBM, CatBoost with custom objectives
    - LINEAR AND KERNEL MODELS (25 lines): ElasticNet, SVR, Ridge, Lasso
    - ENSEMBLE METHODS (15 lines): ExtraTrees, Random Forest, Voting regressor

    2. ADVANCED STACKING ARCHITECTURE (60+ lines)
    - LEVEL-1 ENSEMBLE (25 lines): Train diverse base models with time-series CV
    - LEVEL-2 META-MODEL (20 lines): Linear, Ridge, ElasticNet meta-learners
    - LEVEL-3 FINAL BLENDING (15 lines): Dynamic weights, volatility adjustment

    3. FINANCIAL-SPECIFIC OPTIMIZATION (35+ lines)
    - Custom loss functions, hyperparameter optimization with Optuna

    4. ROBUST VALIDATION FRAMEWORK (25+ lines)
    - Purged time series cross-validation, walk-forward analysis

    5. PRODUCTION DEPLOYMENT (10+ lines)
    - Save final ensemble as model.pkl using joblib
    - Prepare test data files

    EXECUTION REQUIREMENTS:
    1. Write executable Python code in a single script
    2. Use ONLY: sklearn, xgboost, lightgbm, catboost, joblib, numpy, pandas, optuna
    3. Implement proper time-series cross-validation
    4. Prevent data leakage with careful validation splits
    5. Optimize for RMSE while maintaining generalization
    6. Save model.pkl for evaluation stage
    7. Generate X_test.csv and y_test.csv files (from test_features.csv)
    8. Save your script as MODEL.py using PythonTools
    9. Create a summary variable with model performance metrics

    FILES TO CREATE:
    - model.pkl (trained ensemble model)
    - X_test.csv (test features for evaluation)
    - y_test.csv (test targets for evaluation)
    - MODEL.py (your modeling script)

    CROSS-AGENT INTEGRATION: Leverage EDA regime analysis and feature engineering outputs for optimal model configuration.

    PERFORMANCE TARGET: Demonstrate mathematical pathway to RMSE â‰¤ 0.0011.
    """
    
    print("\nðŸ¤– Stage 3: Advanced Ensemble Modeling by Dr. Emily Watson")
    print("ðŸŽ¯ Target: Build ensemble achieving RMSE â‰¤ 0.0011")
    
    modeling_output = run_agent_with_retry(modeling_agent, modeling_prompt)
    
    # Store modeling insights and track files
    modeling_insights = {
        "stage": "Modeling_Complete",
        "ensemble_type": "Advanced stacking with 6+ base models",
        "validation": "Purged time series cross-validation",
        "target_rmse": "â‰¤ 0.0011"
    }
    store_agent_insights("Modeling_Agent", modeling_insights)
    
    # Track files created
    model_files = ["model.pkl", "X_test.csv", "y_test.csv", "MODEL.py"]
    track_created_files("Modeling_Agent", model_files)
    
    run_details.append(("Modeling_Agent",
                        modeling_agent,
                        modeling_prompt,
                        modeling_output))

    # ========== COMPREHENSIVE EVALUATION ==========
    previous_insights = get_previous_insights()
    created_files = get_files_list_string()
    
    evaluation_prompt = f"""
    FINAL VALIDATION: Conduct comprehensive evaluation to prove model superiority and ensure RMSE â‰¤ 0.0011. This determines competition victory.

    PIPELINE INSIGHTS: {json.dumps(previous_insights, indent=2)}
    
    FILES AVAILABLE FROM PREVIOUS AGENTS:
    {created_files}
    
    INPUT: model.pkl, X_test.csv, y_test.csv (created by Modeling_Agent)
    CRITICAL REQUIREMENT: RMSE must be â‰¤ 0.0011 (competition threshold)

    IMPORTANT - PYTHON TOOLS USAGE:
    You must use the PythonTools to write and execute your code. Use the save_to_file_and_run method with:
    - filename: "EVAL.py"
    - variable_to_return: "evaluation_summary" (or another string variable name that contains your evaluation summary)

    ERROR HANDLING REQUIREMENTS:
    - If you encounter ANY error, you MUST fix it and re-run using save_to_file_and_run
    - Load models with proper error handling
    - Check file existence before reading
    - Handle matplotlib errors (remove deprecated parameters)
    - Verify data compatibility before predictions
    - Keep trying until your code runs successfully

    MANDATORY DELIVERABLES (Generate 200+ lines of comprehensive evaluation):

    Your Python script should include:

    1. CORE PERFORMANCE METRICS (50+ lines)
    - PRIMARY METRICS: RMSE, MAE, R-squared, IC, maximum error
    - FINANCIAL METRICS: Sharpe ratio, information ratio, Calmar ratio, Sortino ratio
    - DIRECTIONAL ACCURACY: Hit ratio, precision/recall, F1-score

    2. ADVANCED STATISTICAL DIAGNOSTICS (60+ lines)
    - RESIDUAL ANALYSIS: Autocorrelation tests, heteroscedasticity tests, normality tests
    - STABILITY TESTING: Structural stability tests, rolling window analysis
    - STATISTICAL SIGNIFICANCE: Bootstrap confidence intervals, model comparison tests

    3. FINANCIAL RISK ANALYSIS (40+ lines)
    - Risk metrics: VaR, CVaR, maximum drawdown
    - Performance attribution and factor analysis

    4. ADVANCED VISUALIZATION (30+ lines)
    - Diagnostic plots, financial charts
    - Use matplotlib WITHOUT deprecated parameters (no use_line_collection)

    5. PRODUCTION READINESS ASSESSMENT (20+ lines)
    - Model interpretability, computational efficiency
    - CRITICAL: Save RMSE to MSFT_Score.txt in EXACT format: "RMSE: <value>"

    EXECUTION REQUIREMENTS:
    1. Write executable Python code in a single script
    2. Use ONLY: sklearn, matplotlib, seaborn, numpy, pandas, scipy, statsmodels, joblib
    3. Calculate RMSE with high precision (6+ decimal places)
    4. Save RMSE to MSFT_Score.txt in format: "RMSE: <value>"
    5. Generate professional-quality plots (save as PNG)
    6. Create detailed performance report
    7. Save your script as EVAL.py using PythonTools
    8. Create a summary variable with all evaluation metrics

    FILES TO CREATE:
    - MSFT_Score.txt (RMSE in exact format)
    - Various .png plot files (residuals, predictions, etc.)
    - EVAL.py (your evaluation script)

    SUCCESS CRITERIA:
    - Primary: RMSE â‰¤ 0.0011 (MANDATORY for competition success)
    - Secondary: Statistical significance of outperformance
    - Tertiary: Risk-adjusted returns superiority

    FINAL MISSION: Deliver irrefutable proof of model superiority for competition victory.
    """
    
    print("\nðŸ“Š Stage 4: Comprehensive Evaluation by Dr. James Park")
    print("ðŸŽ¯ Target: Validate RMSE â‰¤ 0.0011 and prove model superiority")
    
    evaluation_output = run_agent_with_retry(evaluation_agent, evaluation_prompt)
    
    # Store final evaluation insights and track files
    eval_insights = {
        "stage": "Evaluation_Complete",
        "rmse_target": "â‰¤ 0.0011",
        "validation_complete": "Comprehensive statistical and financial analysis",
        "production_ready": "Model validated for deployment"
    }
    store_agent_insights("Evaluation_Agent", eval_insights)
    
    # Track files created
    eval_files = ["MSFT_Score.txt", "EVAL.py"]
    track_created_files("Evaluation_Agent", eval_files)
    
    run_details.append(("Evaluation_Agent",
                        evaluation_agent,
                        evaluation_prompt,
                        evaluation_output))

    return run_details


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    load_dotenv()  # Load environment variables
    
    base_dir = Path(".")
    train_csv = "train.csv"
    val_csv   = "val.csv" 
    test_csv  = "test.csv"

    missing = [p for p in (train_csv, val_csv, test_csv) if not Path(p).exists()]
    if missing:
        print("âŒ Missing required files:")
        for p in missing:
            print(f"  - {p}")
        raise SystemExit(1)

    print("ðŸš€ LAUNCHING ADVANCED FINANCIAL PREDICTION COMPETITION PIPELINE")
    print("ðŸ† Target: Achieve RMSE â‰¤ 0.0011 for Competition Victory")
    print("ðŸ§  File-Based Memory System with Cross-Agent Communication")
    print("ðŸ“ Enhanced File Tracking System for Agent Coordination")
    print("ðŸ”„ Automatic Error Recovery and Re-execution")
    print("ðŸ’¼ Competition-Winning Strategies from Top Hedge Funds")
    print("=" * 80)
    
    try:
        # Run advanced pipeline
        agent_runs = run_advanced_pipeline(
            str(train_csv), str(val_csv), str(test_csv))
        
        print("\n" + "=" * 80)
        print("ðŸ“ Generating Competition Submission Log...")
        
        # Build submission log (maintains exact same format)
        submission_logs = generate_submission_log(agent_runs)

        # Verify critical files
        required_files = ["EDA.py", "FEATURE.py", "MODEL.py", "EVAL.py", "MSFT_Score.txt"]
        missing = [f for f in required_files if not Path(f).exists()]
        
        if missing:
            print(f"\nâŒ PIPELINE INCOMPLETE - Missing files: {missing}")
            print("ðŸ”„ Re-running failed stages...")
        else:
            print("\nâœ… COMPETITION PIPELINE SUCCESSFULLY COMPLETED!")
            print("ðŸ† All required files generated for submission")
            
            # Display final results
            try:
                with open("MSFT_Score.txt", "r") as f:
                    final_rmse = f.read().strip()
                    print(f"ðŸŽ¯ FINAL RESULT: {final_rmse}")
                    
                    # Extract numeric value for comparison
                    if ":" in final_rmse:
                        rmse_value = float(final_rmse.split(":")[1].strip())
                        if rmse_value <= 0.0011:
                            print("ðŸŽ‰ SUCCESS! RMSE target achieved - Competition ready!")
                        else:
                            print(f"âš ï¸  RMSE {rmse_value:.6f} exceeds target 0.0011 - Review required")
                    
            except Exception as e:
                print(f"âš ï¸  Could not read final RMSE: {e}")
            
            print("\nðŸ“ Files created by agents:")
            created_files = get_all_created_files()
            for agent, files in created_files.items():
                print(f"  {agent}: {', '.join(files)}")
            
            print("\nðŸ“ Complete execution log saved: submission_log.json")
            print("ðŸ” Agent insights preserved in: agent_insights.json")
            print("ðŸ“‚ File tracking data saved in: created_files_tracker.json")
            
    except Exception as e:
        print(f"\nâŒ PIPELINE ERROR: {e}")
        print("ðŸ”„ Check agent configurations and library dependencies")
        raise