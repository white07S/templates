"""
Advanced Milvus CRUD Manager with Multi-Vector Support
Implements proper multi-vector storage using separate collections and vector fields
"""

import hashlib
import json
import logging
import gzip
import base64
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import pandas as pd
from pymilvus import (
    MilvusClient,
    DataType,
    connections,
    utility
)
from pymilvus.exceptions import MilvusException
import pyarrow.parquet as pq
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class VectorType(Enum):
    """Types of vectors stored in the system"""
    SUMMARY = "summary"
    MATERIALIZE_LOSSES = "materialize_losses"
    CONTROL_FAILURES = "control_failures"


class SearchStrategy(Enum):
    """Search strategies for different use cases"""
    KEYWORD_ONLY = "keyword_only"
    VECTOR_ONLY = "vector_only"
    HYBRID = "hybrid"
    MULTI_VECTOR = "multi_vector"


@dataclass
class SearchConfig:
    """Configuration for search operations"""
    top_k: int = 10
    keyword_weight: float = 0.3
    vector_weights: Dict[str, float] = None
    rerank_strategy: str = "RRF"  # RRF or WeightedScoring
    enable_text_match: bool = True
    consistency_level: str = "Strong"
    timeout: float = 30.0
    search_all_vectors: bool = True
    
    def __post_init__(self):
        if self.vector_weights is None:
            self.vector_weights = {
                VectorType.SUMMARY.value: 0.5,
                VectorType.MATERIALIZE_LOSSES.value: 0.3,
                VectorType.CONTROL_FAILURES.value: 0.2
            }


@dataclass
class IngestionConfig:
    """Configuration for data ingestion"""
    batch_size: int = 100  # Reduced for large vectors
    max_workers: int = 2
    enable_duplicate_check: bool = True
    upsert_mode: bool = True
    retry_attempts: int = 3
    retry_delay: float = 1.0
    compress_json: bool = True
    max_json_size: int = 60000  # Leave buffer for 65536 limit


class MilvusAdvancedManager:
    """
    Advanced Milvus manager with proper multi-vector support
    Uses separate collections for different vector types to handle multiple vectors per entity
    """
    
    # Collection names
    MAIN_COLLECTION = "issues_main"
    VECTOR_COLLECTION = "issues_vectors"
    
    # Field specifications
    VECTOR_DIM = 4096
    MAX_JSON_LENGTH = 65535
    MAX_VARCHAR_LENGTH = 65535
    MAX_VECTORS_PER_TYPE = 10  # Maximum vectors per type per entity
    
    def __init__(
        self,
        uri: str = "http://localhost:19530",
        token: str = "root:Milvus",
        db_name: str = "default"
    ):
        """Initialize Milvus connection"""
        self.uri = uri
        self.token = token
        self.db_name = db_name
        self.client = None
        self._connect()
        
    def _connect(self):
        """Establish connection to Milvus server"""
        try:
            self.client = MilvusClient(
                uri=self.uri,
                token=self.token,
                db_name=self.db_name
            )
            logger.info(f"Connected to Milvus at {self.uri}")
        except Exception as e:
            logger.error(f"Failed to connect to Milvus: {e}")
            raise
    
    def _generate_hash_id(self, raw_meta_data: Dict) -> str:
        """Generate hash ID from raw_meta_data"""
        json_str = json.dumps(raw_meta_data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def _compress_json(self, data: Dict) -> str:
        """Compress JSON data to fit within size limits"""
        json_str = json.dumps(data)
        
        # Check if compression is needed
        if len(json_str.encode('utf-8')) > self.MAX_JSON_LENGTH * 0.9:
            # Compress the data
            compressed = gzip.compress(json_str.encode('utf-8'))
            encoded = base64.b64encode(compressed).decode('utf-8')
            
            # Create compressed wrapper
            compressed_data = {
                "_compressed": True,
                "data": encoded
            }
            return json.dumps(compressed_data)
        
        return json_str
    
    def _decompress_json(self, data: Union[Dict, str]) -> Dict:
        """Decompress JSON data if it was compressed"""
        if isinstance(data, str):
            data = json.loads(data)
        
        if isinstance(data, dict) and data.get("_compressed"):
            # Decompress the data
            decoded = base64.b64decode(data["data"])
            decompressed = gzip.decompress(decoded)
            return json.loads(decompressed.decode('utf-8'))
        
        return data
    
    def create_collections(self, recreate: bool = False) -> bool:
        """
        Create main collection for metadata and vector collection for embeddings
        """
        try:
            # Main collection for entity metadata
            if self.client.has_collection(self.MAIN_COLLECTION):
                if recreate:
                    logger.info(f"Dropping existing collection: {self.MAIN_COLLECTION}")
                    self.client.drop_collection(self.MAIN_COLLECTION)
                else:
                    logger.info(f"Collection {self.MAIN_COLLECTION} already exists")
            
            if not self.client.has_collection(self.MAIN_COLLECTION) or recreate:
                # Create main collection schema
                main_schema = MilvusClient.create_schema(
                    auto_id=False,
                    enable_dynamic_field=False
                )
                
                # Primary key
                main_schema.add_field(
                    field_name="hash_id",
                    datatype=DataType.VARCHAR,
                    max_length=64,
                    is_primary=True
                )
                
                # Metadata fields (compressed if needed)
                main_schema.add_field(
                    field_name="raw_meta_data",
                    datatype=DataType.JSON
                )
                
                main_schema.add_field(
                    field_name="ai_taxonomy",
                    datatype=DataType.JSON
                )
                
                main_schema.add_field(
                    field_name="ai_root_cause",
                    datatype=DataType.JSON
                )
                
                main_schema.add_field(
                    field_name="ai_enrichment",
                    datatype=DataType.JSON
                )
                
                main_schema.add_field(
                    field_name="all_actions",
                    datatype=DataType.JSON
                )
                
                # Summary vector (single vector per entity)
                main_schema.add_field(
                    field_name="summary_embedding",
                    datatype=DataType.FLOAT_VECTOR,
                    dim=self.VECTOR_DIM
                )
                
                # Searchable text for keyword matching
                main_schema.add_field(
                    field_name="keyword_text",
                    datatype=DataType.VARCHAR,
                    max_length=self.MAX_VARCHAR_LENGTH,
                    enable_analyzer=True,
                    enable_match=True
                )
                
                # Timestamps
                main_schema.add_field(
                    field_name="created_at",
                    datatype=DataType.INT64
                )
                
                main_schema.add_field(
                    field_name="updated_at",
                    datatype=DataType.INT64
                )
                
                # Vector counts for tracking multiple vectors
                main_schema.add_field(
                    field_name="vector_counts",
                    datatype=DataType.JSON  # {type: count}
                )
                
                # Create index parameters
                main_index_params = self.client.prepare_index_params()
                
                # Vector index for summary
                main_index_params.add_index(
                    field_name="summary_embedding",
                    index_type="HNSW",
                    metric_type="L2",
                    params={"M": 16, "efConstruction": 256}
                )
                
                # Text index for keyword search
                main_index_params.add_index(
                    field_name="keyword_text",
                    index_type="INVERTED"
                )
                
                # Create main collection
                self.client.create_collection(
                    collection_name=self.MAIN_COLLECTION,
                    schema=main_schema,
                    index_params=main_index_params,
                    consistency_level="Strong"
                )
                
                logger.info(f"Main collection {self.MAIN_COLLECTION} created")
            
            # Vector collection for multiple embeddings per entity
            if self.client.has_collection(self.VECTOR_COLLECTION):
                if recreate:
                    logger.info(f"Dropping existing collection: {self.VECTOR_COLLECTION}")
                    self.client.drop_collection(self.VECTOR_COLLECTION)
                else:
                    logger.info(f"Collection {self.VECTOR_COLLECTION} already exists")
            
            if not self.client.has_collection(self.VECTOR_COLLECTION) or recreate:
                # Create vector collection schema
                vector_schema = MilvusClient.create_schema(
                    auto_id=True,  # Auto-generate IDs for vectors
                    enable_dynamic_field=False
                )
                
                # Auto-generated ID
                vector_schema.add_field(
                    field_name="id",
                    datatype=DataType.INT64,
                    is_primary=True,
                    auto_id=True
                )
                
                # Reference to main entity
                vector_schema.add_field(
                    field_name="entity_hash_id",
                    datatype=DataType.VARCHAR,
                    max_length=64
                )
                
                # Vector type
                vector_schema.add_field(
                    field_name="vector_type",
                    datatype=DataType.VARCHAR,
                    max_length=50
                )
                
                # Vector index within type (for ordered vectors)
                vector_schema.add_field(
                    field_name="vector_index",
                    datatype=DataType.INT32
                )
                
                # The actual embedding
                vector_schema.add_field(
                    field_name="embedding",
                    datatype=DataType.FLOAT_VECTOR,
                    dim=self.VECTOR_DIM
                )
                
                # Metadata for this specific vector
                vector_schema.add_field(
                    field_name="metadata",
                    datatype=DataType.JSON
                )
                
                # Create index parameters
                vector_index_params = self.client.prepare_index_params()
                
                # Vector index
                vector_index_params.add_index(
                    field_name="embedding",
                    index_type="HNSW",
                    metric_type="L2",
                    params={"M": 16, "efConstruction": 256}
                )
                
                # Create vector collection
                self.client.create_collection(
                    collection_name=self.VECTOR_COLLECTION,
                    schema=vector_schema,
                    index_params=vector_index_params,
                    consistency_level="Strong"
                )
                
                logger.info(f"Vector collection {self.VECTOR_COLLECTION} created")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to create collections: {e}")
            raise
    
    def _prepare_main_entity(self, row: pd.Series) -> Dict:
        """Prepare main entity data for insertion"""
        try:
            # Generate hash ID
            hash_id = self._generate_hash_id(row['raw_meta_data'])
            
            # Extract keywords for text search
            keyword_text = self._extract_keywords(row)
            
            # Count vectors for each type
            vector_counts = {
                VectorType.MATERIALIZE_LOSSES.value: len(row.get('issue_materialize_losses_embeddings', [])),
                VectorType.CONTROL_FAILURES.value: len(row.get('issues_failing_from_control_standpoint', []))
            }
            
            # Prepare main entity
            entity = {
                "hash_id": hash_id,
                "raw_meta_data": self._compress_json(row['raw_meta_data']),
                "ai_taxonomy": self._compress_json(row['ai_taxonomy']),
                "ai_root_cause": self._compress_json(row['ai_root_cause']),
                "ai_enrichment": self._compress_json(row['ai_enrichment']),
                "all_actions": self._compress_json(row['all_actions']),
                "summary_embedding": row['summary_embeddings'].tolist() if isinstance(row['summary_embeddings'], np.ndarray) else row['summary_embeddings'],
                "keyword_text": keyword_text[:self.MAX_VARCHAR_LENGTH],
                "created_at": int(time.time() * 1000),
                "updated_at": int(time.time() * 1000),
                "vector_counts": vector_counts
            }
            
            return entity
            
        except Exception as e:
            logger.error(f"Error preparing main entity: {e}")
            raise
    
    def _prepare_vector_entities(self, row: pd.Series, hash_id: str) -> List[Dict]:
        """Prepare vector entities for insertion"""
        vectors = []
        
        try:
            # Process materialize losses embeddings (note: fixing the typo in field name)
            if 'issue_materialize_losses_embeddings' in row:
                for idx, vec in enumerate(row['issue_materialize_losses_embeddings'][:self.MAX_VECTORS_PER_TYPE]):
                    vectors.append({
                        "entity_hash_id": hash_id,
                        "vector_type": VectorType.MATERIALIZE_LOSSES.value,
                        "vector_index": idx,
                        "embedding": vec.tolist() if isinstance(vec, np.ndarray) else vec,
                        "metadata": {"source": "materialize_losses", "index": idx}
                    })
            
            # Process control standpoint failure embeddings
            if 'issues_failing_from_control_standpoint' in row:
                for idx, vec in enumerate(row['issues_failing_from_control_standpoint'][:self.MAX_VECTORS_PER_TYPE]):
                    vectors.append({
                        "entity_hash_id": hash_id,
                        "vector_type": VectorType.CONTROL_FAILURES.value,
                        "vector_index": idx,
                        "embedding": vec.tolist() if isinstance(vec, np.ndarray) else vec,
                        "metadata": {"source": "control_failures", "index": idx}
                    })
            
            return vectors
            
        except Exception as e:
            logger.error(f"Error preparing vector entities: {e}")
            return []
    
    def _extract_keywords(self, row: pd.Series) -> str:
        """Extract searchable keywords from JSON fields"""
        keywords = []
        
        # Extract text from JSON fields
        for field in ['raw_meta_data', 'ai_taxonomy', 'ai_root_cause', 'ai_enrichment', 'all_actions']:
            if field in row and row[field]:
                if isinstance(row[field], dict):
                    # Extract values from dictionary
                    def extract_text(obj, depth=0):
                        if depth > 3:  # Limit recursion depth
                            return []
                        texts = []
                        if isinstance(obj, dict):
                            for k, v in obj.items():
                                texts.append(str(k))
                                texts.extend(extract_text(v, depth + 1))
                        elif isinstance(obj, list):
                            for item in obj:
                                texts.extend(extract_text(item, depth + 1))
                        else:
                            texts.append(str(obj))
                        return texts
                    
                    keywords.extend(extract_text(row[field]))
                else:
                    keywords.append(str(row[field]))
        
        # Join and clean
        keyword_text = " ".join(keywords)
        # Remove excessive whitespace
        keyword_text = " ".join(keyword_text.split())
        
        return keyword_text
    
    def bulk_ingest(
        self,
        parquet_file: str,
        config: Optional[IngestionConfig] = None
    ) -> Dict[str, Any]:
        """
        Bulk ingest data from parquet file
        """
        if config is None:
            config = IngestionConfig()
        
        results = {
            "total_records": 0,
            "main_inserted": 0,
            "vectors_inserted": 0,
            "updated": 0,
            "errors": 0,
            "error_details": []
        }
        
        try:
            # Read parquet file
            logger.info(f"Reading parquet file: {parquet_file}")
            df = pd.read_parquet(parquet_file)
            results["total_records"] = len(df)
            
            # Process in batches
            for batch_start in range(0, len(df), config.batch_size):
                batch_end = min(batch_start + config.batch_size, len(df))
                batch_df = df.iloc[batch_start:batch_end]
                
                main_entities = []
                all_vectors = []
                
                for _, row in batch_df.iterrows():
                    try:
                        # Prepare main entity
                        main_entity = self._prepare_main_entity(row)
                        hash_id = main_entity["hash_id"]
                        
                        # Check for existing entity
                        if config.enable_duplicate_check:
                            existing = self.read(hash_id)
                            if existing and not config.upsert_mode:
                                continue
                            elif existing and config.upsert_mode:
                                # Delete existing for update
                                self.delete(hash_id)
                                results["updated"] += 1
                        
                        main_entities.append(main_entity)
                        
                        # Prepare vector entities
                        vectors = self._prepare_vector_entities(row, hash_id)
                        all_vectors.extend(vectors)
                        
                    except Exception as e:
                        logger.error(f"Error processing row: {e}")
                        results["errors"] += 1
                        results["error_details"].append(str(e))
                
                # Insert main entities
                if main_entities:
                    try:
                        self.client.insert(
                            collection_name=self.MAIN_COLLECTION,
                            data=main_entities
                        )
                        results["main_inserted"] += len(main_entities)
                        logger.info(f"Inserted {len(main_entities)} main entities")
                    except Exception as e:
                        logger.error(f"Error inserting main entities: {e}")
                        results["errors"] += 1
                        results["error_details"].append(str(e))
                
                # Insert vector entities
                if all_vectors:
                    try:
                        # Insert in smaller batches for vectors
                        vector_batch_size = 100
                        for i in range(0, len(all_vectors), vector_batch_size):
                            vector_batch = all_vectors[i:i + vector_batch_size]
                            self.client.insert(
                                collection_name=self.VECTOR_COLLECTION,
                                data=vector_batch
                            )
                            results["vectors_inserted"] += len(vector_batch)
                        
                        logger.info(f"Inserted {len(all_vectors)} vector entities")
                    except Exception as e:
                        logger.error(f"Error inserting vector entities: {e}")
                        results["errors"] += 1
                        results["error_details"].append(str(e))
            
            logger.info(f"Ingestion completed: {results}")
            return results
            
        except Exception as e:
            logger.error(f"Bulk ingestion failed: {e}")
            results["errors"] += 1
            results["error_details"].append(str(e))
            raise
    
    def read(self, hash_id: str) -> Optional[Dict]:
        """Read a complete entity with all its vectors"""
        try:
            # Get main entity
            main_results = self.client.get(
                collection_name=self.MAIN_COLLECTION,
                ids=[hash_id],
                output_fields=["*"]
            )
            
            if not main_results:
                return None
            
            main_entity = main_results[0]
            
            # Decompress JSON fields
            for field in ['raw_meta_data', 'ai_taxonomy', 'ai_root_cause', 'ai_enrichment', 'all_actions']:
                if field in main_entity:
                    main_entity[field] = self._decompress_json(main_entity[field])
            
            # Get associated vectors
            filter_expr = f'entity_hash_id == "{hash_id}"'
            vectors = self.client.query(
                collection_name=self.VECTOR_COLLECTION,
                filter=filter_expr,
                output_fields=["*"]
            )
            
            # Organize vectors by type
            main_entity['vectors'] = {
                VectorType.MATERIALIZE_LOSSES.value: [],
                VectorType.CONTROL_FAILURES.value: []
            }
            
            for vec in vectors:
                vec_type = vec.get('vector_type')
                if vec_type in main_entity['vectors']:
                    main_entity['vectors'][vec_type].append(vec)
            
            # Sort vectors by index
            for vec_type in main_entity['vectors']:
                main_entity['vectors'][vec_type].sort(key=lambda x: x.get('vector_index', 0))
            
            return main_entity
            
        except Exception as e:
            logger.error(f"Error reading entity {hash_id}: {e}")
            raise
    
    def update(self, hash_id: str, data: Dict) -> bool:
        """Update an entity and its vectors"""
        try:
            # Delete existing entity and vectors
            self.delete(hash_id)
            
            # Prepare updated data as DataFrame row
            row = pd.Series(data)
            
            # Insert main entity
            main_entity = self._prepare_main_entity(row)
            main_entity["updated_at"] = int(time.time() * 1000)
            
            self.client.insert(
                collection_name=self.MAIN_COLLECTION,
                data=[main_entity]
            )
            
            # Insert vector entities
            vectors = self._prepare_vector_entities(row, hash_id)
            if vectors:
                self.client.insert(
                    collection_name=self.VECTOR_COLLECTION,
                    data=vectors
                )
            
            logger.info(f"Updated entity: {hash_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error updating entity {hash_id}: {e}")
            raise
    
    def delete(self, hash_id: Union[str, List[str]]) -> bool:
        """Delete entity and all associated vectors"""
        try:
            if isinstance(hash_id, str):
                hash_ids = [hash_id]
            else:
                hash_ids = hash_id
            
            # Delete from main collection
            filter_expr = f'hash_id in {hash_ids}'
            self.client.delete(
                collection_name=self.MAIN_COLLECTION,
                filter=filter_expr
            )
            
            # Delete from vector collection
            for hid in hash_ids:
                vector_filter = f'entity_hash_id == "{hid}"'
                self.client.delete(
                    collection_name=self.VECTOR_COLLECTION,
                    filter=vector_filter
                )
            
            logger.info(f"Deleted {len(hash_ids)} entities")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting entities: {e}")
            raise
    
    def advanced_search(
        self,
        keywords: Optional[List[str]] = None,
        embeddings: Optional[List[np.ndarray]] = None,
        config: Optional[SearchConfig] = None
    ) -> List[Dict]:
        """
        Advanced multi-vector search with keyword matching
        """
        if config is None:
            config = SearchConfig()
        
        try:
            # Load collections
            self.client.load_collection(self.MAIN_COLLECTION)
            self.client.load_collection(self.VECTOR_COLLECTION)
            
            results = {}
            
            # Keyword search
            if keywords:
                keyword_results = self._keyword_search(keywords, config)
                for res in keyword_results:
                    hash_id = res['hash_id']
                    if hash_id not in results:
                        results[hash_id] = res
                        results[hash_id]['scores'] = {}
                    results[hash_id]['scores']['keyword'] = res.get('keyword_score', 0)
            
            # Vector searches
            if embeddings:
                for idx, embedding in enumerate(embeddings):
                    # Search in summary embeddings
                    summary_results = self._search_summary_vectors(embedding, config)
                    for res in summary_results:
                        hash_id = res['id']
                        if hash_id not in results:
                            results[hash_id] = {'hash_id': hash_id, 'scores': {}}
                        results[hash_id]['scores'][f'summary_{idx}'] = 1.0 / (1.0 + res.get('distance', 1.0))
                    
                    # Search in multiple vector collections
                    if config.search_all_vectors:
                        multi_results = self._search_multiple_vectors(embedding, config)
                        for res in multi_results:
                            hash_id = res['entity_hash_id']
                            if hash_id not in results:
                                results[hash_id] = {'hash_id': hash_id, 'scores': {}}
                            vec_type = res.get('vector_type', 'unknown')
                            score_key = f'{vec_type}_{idx}'
                            current_score = results[hash_id]['scores'].get(score_key, 0)
                            new_score = 1.0 / (1.0 + res.get('distance', 1.0))
                            results[hash_id]['scores'][score_key] = max(current_score, new_score)
            
            # Calculate final scores and rank
            final_results = self._rerank_results(results, config)
            
            # Fetch full data for top results
            top_results = []
            for result in final_results[:config.top_k]:
                full_data = self.read(result['hash_id'])
                if full_data:
                    full_data['search_scores'] = result.get('scores', {})
                    full_data['final_score'] = result.get('final_score', 0)
                    top_results.append(full_data)
            
            return top_results
            
        except Exception as e:
            logger.error(f"Advanced search failed: {e}")
            raise
    
    def _keyword_search(self, keywords: List[str], config: SearchConfig) -> List[Dict]:
        """Perform keyword search"""
        try:
            # Build filter expression
            keyword_filters = []
            for keyword in keywords:
                keyword_filters.append(f'TEXT_MATCH(keyword_text, "{keyword}")')
            
            filter_expr = " or ".join(keyword_filters)
            
            # Query
            results = self.client.query(
                collection_name=self.MAIN_COLLECTION,
                filter=filter_expr,
                output_fields=["hash_id", "raw_meta_data"],
                limit=config.top_k * 2  # Get more for scoring
            )
            
            # Calculate keyword scores
            for result in results:
                score = 0
                raw_data = self._decompress_json(result.get('raw_meta_data', {}))
                text = json.dumps(raw_data).lower()
                for keyword in keywords:
                    score += text.count(keyword.lower())
                result['keyword_score'] = score
            
            # Sort by score
            results.sort(key=lambda x: x['keyword_score'], reverse=True)
            
            return results[:config.top_k]
            
        except Exception as e:
            logger.error(f"Keyword search failed: {e}")
            return []
    
    def _search_summary_vectors(self, embedding: np.ndarray, config: SearchConfig) -> List[Dict]:
        """Search in summary embeddings"""
        try:
            results = self.client.search(
                collection_name=self.MAIN_COLLECTION,
                data=[embedding.tolist()],
                anns_field="summary_embedding",
                limit=config.top_k,
                output_fields=["hash_id"]
            )
            return results[0] if results else []
            
        except Exception as e:
            logger.error(f"Summary vector search failed: {e}")
            return []
    
    def _search_multiple_vectors(self, embedding: np.ndarray, config: SearchConfig) -> List[Dict]:
        """Search in multiple vector collection"""
        try:
            results = self.client.search(
                collection_name=self.VECTOR_COLLECTION,
                data=[embedding.tolist()],
                anns_field="embedding",
                limit=config.top_k * 2,  # Get more since we'll aggregate
                output_fields=["entity_hash_id", "vector_type", "vector_index"]
            )
            return results[0] if results else []
            
        except Exception as e:
            logger.error(f"Multi-vector search failed: {e}")
            return []
    
    def _rerank_results(self, results: Dict, config: SearchConfig) -> List[Dict]:
        """Rerank results based on configuration"""
        for hash_id, result in results.items():
            scores = result.get('scores', {})
            
            if config.rerank_strategy == "WeightedScoring":
                # Weighted scoring
                final_score = 0
                
                # Keyword score
                if 'keyword' in scores:
                    final_score += config.keyword_weight * scores['keyword']
                
                # Vector scores with type-specific weights
                for score_key, score_val in scores.items():
                    if score_key != 'keyword':
                        # Determine vector type
                        for vec_type in config.vector_weights:
                            if vec_type in score_key:
                                final_score += config.vector_weights[vec_type] * score_val
                                break
                
                result['final_score'] = final_score
                
            else:  # RRF
                # Reciprocal Rank Fusion
                k = 60
                rrf_score = 0
                
                # Create rank lists
                rank_lists = {}
                
                # Keyword rank
                if 'keyword' in scores:
                    keyword_sorted = sorted(
                        [(hid, r['scores'].get('keyword', 0)) for hid, r in results.items()],
                        key=lambda x: x[1],
                        reverse=True
                    )
                    for rank, (hid, _) in enumerate(keyword_sorted, 1):
                        if hid == hash_id:
                            rrf_score += 1.0 / (k + rank)
                            break
                
                # Vector ranks
                for score_type in ['summary', 'materialize_losses', 'control_failures']:
                    type_scores = [(hid, max([r['scores'].get(key, 0) 
                                             for key in r['scores'] 
                                             if score_type in key], default=0))
                                  for hid, r in results.items()]
                    type_sorted = sorted(type_scores, key=lambda x: x[1], reverse=True)
                    
                    for rank, (hid, score) in enumerate(type_sorted, 1):
                        if hid == hash_id and score > 0:
                            rrf_score += 1.0 / (k + rank)
                            break
                
                result['final_score'] = rrf_score
        
        # Sort by final score
        ranked_results = sorted(
            results.values(),
            key=lambda x: x.get('final_score', 0),
            reverse=True
        )
        
        return ranked_results
    
    def get_statistics(self) -> Dict:
        """Get collection statistics"""
        try:
            stats = {
                "main_collection": {
                    "name": self.MAIN_COLLECTION,
                    "entity_count": self.client.get_collection_stats(self.MAIN_COLLECTION).get("row_count", 0)
                },
                "vector_collection": {
                    "name": self.VECTOR_COLLECTION,
                    "entity_count": self.client.get_collection_stats(self.VECTOR_COLLECTION).get("row_count", 0)
                }
            }
            
            # Get vector type distribution
            try:
                vector_types = self.client.query(
                    collection_name=self.VECTOR_COLLECTION,
                    filter="",
                    output_fields=["vector_type"],
                    limit=10000
                )
                
                type_counts = {}
                for vec in vector_types:
                    vtype = vec.get('vector_type')
                    type_counts[vtype] = type_counts.get(vtype, 0) + 1
                
                stats["vector_distribution"] = type_counts
                
            except:
                pass
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get statistics: {e}")
            raise


# Example usage
if __name__ == "__main__":
    # Initialize manager
    manager = MilvusAdvancedManager()
    
    # Create collections
    manager.create_collections(recreate=True)
    
    # Example: Ingest data
    # config = IngestionConfig(
    #     batch_size=50,  # Smaller batches for large vectors
    #     enable_duplicate_check=True,
    #     upsert_mode=True,
    #     compress_json=True
    # )
    # results = manager.bulk_ingest("issues_data.parquet", config)
    # print(f"Ingestion results: {results}")
    
    # Example: Advanced search
    # search_config = SearchConfig(
    #     top_k=20,
    #     keyword_weight=0.3,
    #     vector_weights={
    #         VectorType.SUMMARY.value: 0.5,
    #         VectorType.MATERIALIZE_LOSSES.value: 0.3,
    #         VectorType.CONTROL_FAILURES.value: 0.2
    #     },
    #     rerank_strategy="RRF",
    #     search_all_vectors=True
    # )
    # 
    # keywords = ["error", "system", "failure"]
    # embeddings = [np.random.randn(4096), np.random.randn(4096)]
    # 
    # results = manager.advanced_search(
    #     keywords=keywords,
    #     embeddings=embeddings,
    #     config=search_config
    # )
    # 
    # for i, result in enumerate(results[:5]):
    #     print(f"Result {i+1}: {result['hash_id']}, Score: {result.get('final_score', 0):.4f}")
    
    # Get statistics
    stats = manager.get_statistics()
    print(f"Collection statistics: {json.dumps(stats, indent=2)}")
