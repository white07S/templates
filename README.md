You‚Äôre describing a classic latency-vs-accuracy trade-off in tool routing. Don‚Äôt let a ‚Äúplanner LLM‚Äù sit on the hot path for trivial turns. Use a staged router with early-exit and confidence thresholds.

Here‚Äôs a design that works well in practice and avoids ‚Äúauto tool choice‚Äù while still being snappy:

0) The principle
	‚Ä¢	Fast path first: cheap, deterministic checks handle greetings/acks/short chit-chat.
	‚Ä¢	Shallow scoring second: lightweight lexical + keyword scoring proposes 0‚ÄìN tool candidates.
	‚Ä¢	LLM planner last (only when ambiguous or multi-tool): a small model decides final need_tools and the minimal set/order.

1) Minimal features to extract (cheap)

From the normalized user text:
	‚Ä¢	tok_len, has_question, has_code_block, has_url, has_numbers
	‚Ä¢	imperatives_hit (e.g., fetch, search, compare, summarize, extract, translate, convert, call, run)
	‚Ä¢	tool_keywords_hit[tool] (per-tool synonym lexicon)
	‚Ä¢	Conversation state flags (e.g., ‚Äúawaiting data upload‚Äù, ‚Äúprevious tool pending‚Äù)

2) Early-exit guard (no LLM)

If all:
	‚Ä¢	tok_len ‚â§ 3 and
	‚Ä¢	matches a small-talk set (hi/hello/thanks/ok/üëç/bye/etc.) and
	‚Ä¢	not in any tool keyword whitelist
‚Üí NO_TOOLS (reply directly).

Also early-exit WITH tool for obvious patterns:
	‚Ä¢	Has URL ‚Üí web_retriever or pdf_loader
	‚Ä¢	Has code block ‚Üí code_runner or linter
	‚Ä¢	Starts with verbs like ‚Äútranslate/summarize/extract‚Äù ‚Üí respective single tool

3) Shallow tool scoring (no LLM)

Score each tool with a cheap hybrid:

score(tool) = 0.5 * Jaccard(query_tokens, tool.desc_tokens)
            + 0.4 * keyword_hits(tool)
            + 0.1 * char_ngrams_overlap

Pick Top-K where score ‚â• Œ∏_low.
	‚Ä¢	If none ‚â• Œ∏_low ‚Üí NO_TOOLS (pure chat/QA).
	‚Ä¢	If one ‚â• Œ∏_high and tok_len < short_cap ‚Üí ONE_TOOL (skip planner).
	‚Ä¢	Else ‚Üí ambiguous ‚Üí go to planner.

Use gap test: if (top1 ‚àí top2) ‚â• Œî, accept top1 without planner.

4) Cheap LLM planner (only when needed)

Prompt a small model (local 3B‚Äì7B is fine) with only the query + the Top-K tool cards (name, purpose, inputs, examples). Constrain to a strict JSON schema:

{
  "need_tools": true,
  "tools": [
    {"name": "pdf_loader", "why": "query includes a PDF url"},
    {"name": "ner_extractor", "why": "user asked to extract entities"}
  ],
  "order": ["pdf_loader", "ner_extractor"],
  "notes": "Return minimal spans only."
}

Put tight caps on max_tokens and a short timeout; if it times out, fall back to Top-1 from the shallow scorer.

5) Caching & state
	‚Ä¢	Cache the router output for the last N turns per user (norm(query) ‚Üí decision) with short TTL.
	‚Ä¢	If the previous turn involved a tool and the new turn is anaphoric (‚Äúand for 2023?‚Äù), reuse previous tool unless shallow scorer contradicts.

6) Example implementation (to-the-point, no external deps)

import re
from typing import List, Dict, Any, Tuple

SMALLTALK = {
    "hi","hello","hey","thanks","thank you","ok","okay","cool","great",
    "nice","yo","sup","bye","goodbye","see ya","ciao","bravo","üëç","üëå","üëã"
}
IMPERATIVES = {"fetch","search","compare","summarize","extract","translate","convert","run","call","plot","download"}

Tool = Dict[str, Any]
tools_catalog: List[Tool] = [
    {
        "name": "web_retriever",
        "desc": "Search and fetch web pages or APIs by URL or query, then return text.",
        "keywords": {"http","https","url","web","site","search","google","fetch"}
    },
    {
        "name": "pdf_loader",
        "desc": "Load a PDF from a URL or bytes and return clean text and pages.",
        "keywords": {"pdf","document","pages","file","report"}
    },
    {
        "name": "code_runner",
        "desc": "Execute user-provided code safely and return stdout/stderr.",
        "keywords": {"code","python","run","execute","stacktrace","error"}
    },
    {
        "name": "ner_extractor",
        "desc": "Extract entities (people, orgs, places) from given text.",
        "keywords": {"extract","entities","people","orgs","locations","named","NER"}
    },
    # add more tools...
]

def norm(text: str) -> str:
    return re.sub(r"\s+", " ", text.strip().lower())

def tokens(text: str) -> List[str]:
    return re.findall(r"[a-z0-9]+", text.lower())

def jaccard(a: List[str], b: List[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa or not sb: return 0.0
    return len(sa & sb) / len(sa | sb)

def char_ngrams(s: str, n=3) -> set:
    s = re.sub(r"\s+", " ", s)
    return {s[i:i+n] for i in range(len(s)-n+1)} if len(s) >= n else set()

def shallow_scores(query: str, catalog: List[Tool]) -> List[Tuple[str, float]]:
    q_toks = tokens(query)
    q_ngr = char_ngrams(query)
    out = []
    for t in catalog:
        desc_toks = tokens(t["desc"])
        kw_hits = len(set(q_toks) & t["keywords"])
        score = (
            0.5 * jaccard(q_toks, desc_toks) +
            0.4 * (kw_hits > 0) +
            0.1 * (len(q_ngr & char_ngrams(t["desc"])) > 0)
        )
        out.append((t["name"], float(score)))
    return sorted(out, key=lambda x: x[1], reverse=True)

def early_exit(query: str) -> Dict[str, Any] | None:
    q = norm(query)
    if len(tokens(q)) <= 3 and (q in SMALLTALK or any(q == s or q.startswith(s) for s in SMALLTALK)):
        return {"need_tools": False, "why": "smalltalk", "tools": []}
    if "```" in query or "code" in tokens(q):
        return {"need_tools": True, "why": "code block present", "tools": ["code_runner"], "order": ["code_runner"]}
    if "http://" in q or "https://" in q:
        # crude PDF vs web split
        return {"need_tools": True, "why": "url present", "tools": (["pdf_loader"] if ".pdf" in q else ["web_retriever"]), "order": (["pdf_loader"] if ".pdf" in q else ["web_retriever"])}
    # imperative single-intent fast path
    if any(w in q for w in IMPERATIVES) and len(tokens(q)) < 15:
        # let shallow scorer pick the one tool
        return None
    return None

def route(query: str, Œ∏_low=0.15, Œ∏_high=0.45, Œî=0.20, short_cap=12) -> Dict[str, Any]:
    ee = early_exit(query)
    if ee is not None:
        return ee

    scores = shallow_scores(query, tools_catalog)
    top1, s1 = scores[0]
    top2, s2 = scores[1] if len(scores) > 1 else (None, 0.0)

    if s1 < Œ∏_low:
        return {"need_tools": False, "why": "scores below Œ∏_low", "tools": []}

    q_len = len(tokens(query))
    if (s1 >= Œ∏_high and (s1 - s2) >= Œî) or (q_len <= short_cap and s1 >= Œ∏_high):
        return {"need_tools": True, "why": "confident single-tool match", "tools": [top1], "order": [top1], "scores": scores[:3]}

    # Ambiguous ‚Üí hand off to planner (small LLM) with only Top-K tool cards
    topK = [name for name, _ in scores[:3]]
    return {
        "need_tools": "planner",
        "why": "ambiguous tool choice; escalate to small planner",
        "candidates": topK,
        "scores": scores[:3],
        "planner_schema": {
            "type": "object",
            "properties": {
                "need_tools": {"type":"boolean"},
                "tools": {"type":"array","items":{"type":"object","properties":{"name":{"type":"string"},"why":{"type":"string"}}}},
                "order": {"type":"array","items":{"type":"string"}},
                "notes": {"type":"string"}
            },
            "required": ["need_tools","tools","order"]
        }
    }

# --- quick demos ---
# print(route("hello"))
# print(route("fetch this https://example.com/abc.pdf"))
# print(route("extract people and orgs from this text"))

What you get with this:
	‚Ä¢	‚Äúhello‚Äù ‚Üí immediate NO_TOOLS.
	‚Ä¢	URL/PDF/code ‚Üí immediate ONE_TOOL.
	‚Ä¢	Short imperative with clear keywords ‚Üí ONE_TOOL via shallow scorer.
	‚Ä¢	Confusing/multi-intent ‚Üí planner only then.

7) Practical knobs
	‚Ä¢	Start with Œ∏_low=0.15, Œ∏_high=0.45, Œî=0.20, short_cap=12. Tune on your logs.
	‚Ä¢	Maintain a per-tool synonym lexicon‚Äîthat‚Äôs the highest ROI.
	‚Ä¢	Put a hard time budget around the planner; on timeout, pick Top-1.

8) When to invoke full ReAct

Gate ReAct behind:
	‚Ä¢	tok_len ‚â• 15 and (has_question or imperatives_hit ‚â• 1)
	‚Ä¢	More than one tool required or tool requires multi-step transformation (e.g., pdf_loader ‚Üí ner_extractor ‚Üí web_retriever).
Otherwise, do a single tool call with a constrained prompt and skip scratch-pad reasoning.

9) Upgrades (when you want more accuracy without latency pain)
	‚Ä¢	Replace shallow scorer with BM25 + embeddings combo and RRF ranker (still cheap).
	‚Ä¢	Add tool co-usage graph to avoid silly multi-tool combos (e.g., pdf_loader pairs with ner_extractor, not code_runner).
	‚Ä¢	Add per-user short-term memory to resolve anaphora and keep the same tool for follow-ups.

This gives you a crisp, low-latency router that avoids burning LLM cycles for ‚Äúhello,‚Äù but still escalates gracefully when a real multi-tool plan is needed.