# .env.example - Example environment configuration for vLLM Proxy
# Copy this to .env and adjust values as needed

# ===== vLLM Model Configuration =====
# Model to serve (HuggingFace model ID or local path)
VLLM_MODEL="Qwen/Qwen-32B-Chat"

# Number of GPUs for tensor parallelism
TENSOR_PARALLEL_SIZE=2

# Data type for model weights
DTYPE="bfloat16"

# GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTILIZATION=0.9

# ===== vLLM Server Settings =====
# Host and port for vLLM backend
VLLM_HOST="127.0.0.1"
VLLM_PORT=8000

# Optional API key for vLLM authentication
# VLLM_API_KEY="your-secure-api-key"

# Enable performance features
ENABLE_CHUNKED_PREFILL="true"
ENABLE_PREFIX_CACHING="true"

# ===== Proxy Server Settings =====
# Host and port for proxy server
PROXY_HOST="0.0.0.0"
PROXY_PORT=9000

# ===== Rate Limiting =====
# Enable/disable rate limiting
RATE_LIMIT_ENABLED="true"

# Maximum requests per minute per IP
RATE_LIMIT_PER_MINUTE=60

# ===== Timeouts (seconds) =====
# Maximum time to wait for vLLM to start
STARTUP_TIMEOUT=300

# Request timeout for API calls
REQUEST_TIMEOUT=300

# Interval between health checks
HEALTH_CHECK_INTERVAL=30

# ===== Retry Configuration =====
# Number of retry attempts for failed requests
MAX_RETRIES=3

# Delay between retries (seconds)
RETRY_DELAY=1.0

# ===== CORS Settings =====
# Allowed origins (comma-separated, use * for all)
CORS_ORIGINS="*"

# ===== Debug Settings =====
# Enable debug mode (shows detailed errors)
DEBUG="false"

# ===== Logging =====
# Log file path (default: vllm_proxy.log)
# LOG_FILE="vllm_proxy.log"

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL="INFO"