import asyncio

# Graphiti core
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMClient, LLMConfig
from graphiti_core.embedder import EmbedderClient
from graphiti_core.cross_encoder import CrossEncoderClient

# LangChain wrappers
from langchain.chat_models import AzureChatOpenAI
from langchain_openai import AzureOpenAIEmbeddings

# ── 1) LangChain → Graphiti LLM Adapter ────────────────────────────────────
class LangChainLLMClient(LLMClient):
    def __init__(self, lc_model: AzureChatOpenAI, config: LLMConfig):
        super().__init__(config)
        self.lc_model = lc_model

    async def chat_complete(self, prompt: str, **kwargs) -> str:
        result = await self.lc_model.agenerate([{"role": "user", "content": prompt}])
        return result.generations[0][0].text

# ── 2) LangChain → Graphiti Embedder Adapter ────────────────────────────────
class LangChainEmbedder(EmbedderClient):
    def __init__(self, lc_embedder: AzureOpenAIEmbeddings):
        super().__init__()
        self.lc_embedder = lc_embedder

    async def create_batch(self, input_data_list: list[str]) -> list[list[float]]:
        # Note: LangChain’s embed_documents returns List[List[float]]
        return self.lc_embedder.embed_documents(input_data_list)

# ── 3) (Optional) LangChain Cross-Encoder via Chat ───────────────────────────
class LangChainCrossEncoder(CrossEncoderClient):
    def __init__(self, lc_model: AzureChatOpenAI):
        self.lc_model = lc_model

    async def rank(self, query: str, passages: list[str]) -> list[tuple[str, float]]:
        # Simple reranking: treat each passage as a yes/no question
        results = []
        for passage in passages:
            prompt = (
                f'PASSAGE: "{passage}"\n\n'
                f'Is this passage relevant to the query: "{query}"? '
                'Respond with "True" or "False".'
            )
            resp = await self.lc_model.agenerate([{"role":"user","content":prompt}])
            text = resp.generations[0][0].text.strip().lower()
            score = 1.0 if text.startswith("true") else 0.0
            results.append((passage, score))
        # Sort descending by score
        return sorted(results, key=lambda x: x[1], reverse=True)

# ── 4) Initialize Graphiti with Adapters ─────────────────────────────────────
async def main():
    # LangChain instances pick up AZURE_ env vars under the hood
    lc_chat  = AzureChatOpenAI(temperature=0)
    lc_embed = AzureOpenAIEmbeddings(model="text-embedding-3-large")

    graphiti = Graphiti(
        uri="bolt://localhost:7687",
        user="neo4j",
        password="neo4j",
        llm_client     = LangChainLLMClient(lc_chat, config=LLMConfig()),
        embedder       = LangChainEmbedder(lc_embed),
        cross_encoder  = LangChainCrossEncoder(lc_chat),
    )

    # Sanity check
    ep = graphiti.create_episode(group_name="demo", data="Testing LangChain adapters.")
    print("Episode created:", ep)

    hits = graphiti.search(
        recipe="semantic_node_search",
        recipe_kwargs={"node_label": None, "query": "Testing adapters"}
    )
    print("Search hits:", hits)

if __name__ == "__main__":
    asyncio.run(main())
