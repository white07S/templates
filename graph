To configure Crush for vLLM (which provides an OpenAI-compatible endpoint), you'll need to create a custom provider configuration. Here's how:
1. Create the Configuration File
bashmkdir -p ~/.config/crush
nano ~/.config/crush/crush.json
2. Configure vLLM Provider
Add this configuration to your crush.json:
json{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "vllm": {
      "type": "openai",
      "base_url": "http://localhost:8000/v1",
      "api_key": "dummy",
      "models": [
        {
          "id": "your-model-name",
          "name": "Your Model Display Name",
          "cost_per_1m_in": 0,
          "cost_per_1m_out": 0,
          "cost_per_1m_in_cached": 0,
          "cost_per_1m_out_cached": 0,
          "context_window": 32768,
          "default_max_tokens": 4096
        }
      ]
    }
  }
}
3. Common vLLM Configurations
For local vLLM with default settings:
json{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "vllm-local": {
      "type": "openai",
      "base_url": "http://localhost:8000/v1",
      "api_key": "dummy",
      "models": [
        {
          "id": "meta-llama/Llama-3.2-3B-Instruct",
          "name": "Llama 3.2 3B Instruct",
          "cost_per_1m_in": 0,
          "cost_per_1m_out": 0,
          "context_window": 8192,
          "default_max_tokens": 2048
        }
      ]
    }
  }
}
For remote vLLM server:
json{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "vllm-remote": {
      "type": "openai",
      "base_url": "http://your-server-ip:8000/v1",
      "api_key": "$VLLM_API_KEY",
      "models": [
        {
          "id": "your-model-name",
          "name": "Your Remote Model",
          "cost_per_1m_in": 0,
          "cost_per_1m_out": 0,
          "context_window": 32768,
          "default_max_tokens": 4096
        }
      ]
    }
  }
}
For vLLM with authentication:
json{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "vllm-auth": {
      "type": "openai",
      "base_url": "http://localhost:8000/v1",
      "api_key": "$VLLM_API_KEY",
      "extra_headers": {
        "Authorization": "Bearer $VLLM_TOKEN"
      },
      "models": [
        {
          "id": "your-model-name",
          "name": "Your Authenticated Model",
          "cost_per_1m_in": 0,
          "cost_per_1m_out": 0,
          "context_window": 32768,
          "default_max_tokens": 4096
        }
      ]
    }
  }
}
4. Key Configuration Parameters
Adjust these based on your vLLM setup:

base_url: Your vLLM server endpoint (default: http://localhost:8000/v1)
api_key: Use "dummy" for local no-auth setups, or "$VLLM_API_KEY" if you have authentication
id: The exact model name as it appears in vLLM (check with curl http://localhost:8000/v1/models)
context_window: Your model's context length
default_max_tokens: Maximum tokens to generate

5. Find Your Model Name
To get the exact model ID that vLLM is serving:
bashcurl http://localhost:8000/v1/models
This will return something like:
json{
  "object": "list",
  "data": [
    {
      "id": "meta-llama/Llama-3.2-3B-Instruct",
      "object": "model",
      ...
    }
  ]
}
Use the "id" value in your configuration.
6. Set Environment Variables (if needed)
bash# Only if your vLLM requires authentication
export VLLM_API_KEY="your-api-key"
export VLLM_TOKEN="your-bearer-token"
7. Test the Configuration
bashcrush
Then in Crush, you should be able to select your vLLM model from the provider list.
8. Common vLLM Ports

Default: 8000
Alternative: 8001, 11434 (for Ollama compatibility)

Make sure your base_url matches your vLLM server's actual endpoint!RetryClaude can make mistakes. Please double-check responses.
