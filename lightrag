import os
import asyncio
import numpy as np
from lightrag.llm.openai import openai_complete_if_cache, openai_embed

# vLLM Configuration
# Chat endpoint (port 8000)
VLLM_CHAT_BASE_URL = "http://localhost:8000/v1"
VLLM_CHAT_API_KEY = "EMPTY"  # vLLM doesn't require real API key for local servers
VLLM_CHAT_MODEL = "your-chat-model-name"  # Replace with your actual model name

# Embedding endpoint (port 8001) 
VLLM_EMBEDDING_BASE_URL = "http://localhost:8001/v1"
VLLM_EMBEDDING_API_KEY = "EMPTY"  # vLLM doesn't require real API key for local servers
VLLM_EMBEDDING_MODEL = "your-embedding-model-name"  # Replace with your actual embedding model name

async def test_llm_function():
    """Test the openai_complete_if_cache function with vLLM chat endpoint"""
    print("üß™ Testing vLLM Chat Completion Function...")
    
    try:
        # Test basic completion
        prompt = "What is artificial intelligence? Please provide a brief explanation."
        
        result = await openai_complete_if_cache(
            model=VLLM_CHAT_MODEL,
            prompt=prompt,
            api_key=VLLM_CHAT_API_KEY,
            base_url=VLLM_CHAT_BASE_URL,
            max_tokens=150,
            temperature=0.7
        )
        
        print("‚úÖ vLLM Chat Function Test PASSED")
        print(f"üìù Response: {result}")
        print(f"üìä Response length: {len(result)} characters")
        return True
        
    except Exception as e:
        print(f"‚ùå vLLM Chat Function Test FAILED: {e}")
        print(f"üí° Make sure your vLLM chat server is running on {VLLM_CHAT_BASE_URL}")
        print(f"üí° Check if model name '{VLLM_CHAT_MODEL}' is correct")
        return False

async def test_embedding_function():
    """Test the openai_embed function with vLLM embedding endpoint"""
    print("\nüß™ Testing vLLM Embedding Function...")
    
    try:
        # Test embedding generation
        test_texts = [
            "This is a test sentence for embedding.",
            "Machine learning is a subset of artificial intelligence.", 
            "Python is a popular programming language."
        ]
        
        embeddings = await openai_embed(
            texts=test_texts,
            model=VLLM_EMBEDDING_MODEL,
            api_key=VLLM_EMBEDDING_API_KEY,
            base_url=VLLM_EMBEDDING_BASE_URL
        )
        
        print("‚úÖ vLLM Embedding Function Test PASSED")
        print(f"üìä Embeddings shape: {embeddings.shape}")
        print(f"üìä Number of texts: {len(test_texts)}")
        print(f"üìä Embedding dimension: {embeddings.shape[1]}")
        print(f"üìä Data type: {embeddings.dtype}")
        
        # Test embedding similarity (cosine similarity between first two embeddings)
        from numpy.linalg import norm
        
        def cosine_similarity(a, b):
            return np.dot(a, b) / (norm(a) * norm(b))
        
        similarity = cosine_similarity(embeddings[0], embeddings[1])
        print(f"üìä Cosine similarity between first two embeddings: {similarity:.4f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå vLLM Embedding Function Test FAILED: {e}")
        print(f"üí° Make sure your vLLM embedding server is running on {VLLM_EMBEDDING_BASE_URL}")
        print(f"üí° Check if model name '{VLLM_EMBEDDING_MODEL}' is correct")
        return False

async def test_embedding_with_custom_function():
    """Test the embedding function as it would be used in LightRAG"""
    print("\nüß™ Testing Custom vLLM Embedding Function...")
    
    async def embedding_func(texts: list[str]) -> np.ndarray:
        return await openai_embed(
            texts,
            model=VLLM_EMBEDDING_MODEL,
            api_key=VLLM_EMBEDDING_API_KEY,
            base_url=VLLM_EMBEDDING_BASE_URL
        )
    
    try:
        test_texts = ["Hello world", "How are you?"]
        result = await embedding_func(test_texts)
        
        print("‚úÖ Custom vLLM Embedding Function Test PASSED")
        print(f"üìä Result shape: {result.shape}")
        return True
        
    except Exception as e:
        print(f"‚ùå Custom vLLM Embedding Function Test FAILED: {e}")
        return False

async def test_llm_with_system_prompt():
    """Test vLLM LLM function with system prompt and history"""
    print("\nüß™ Testing vLLM Chat with System Prompt...")
    
    try:
        system_prompt = "You are a helpful assistant that provides concise answers."
        history_messages = [
            {"role": "user", "content": "What is Python?"},
            {"role": "assistant", "content": "Python is a high-level programming language."}
        ]
        prompt = "Can you tell me more about its uses?"
        
        result = await openai_complete_if_cache(
            model=VLLM_CHAT_MODEL,
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=VLLM_CHAT_API_KEY,
            base_url=VLLM_CHAT_BASE_URL,
            max_tokens=100,
            temperature=0.7
        )
        
        print("‚úÖ vLLM Chat with System Prompt Test PASSED")
        print(f"üìù Response: {result}")
        return True
        
    except Exception as e:
        print(f"‚ùå vLLM Chat with System Prompt Test FAILED: {e}")
        return False

async def test_server_connectivity():
    """Test if vLLM servers are reachable"""
    print("\nüîç Testing vLLM Server Connectivity...")
    
    import aiohttp
    
    # Test chat server
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{VLLM_CHAT_BASE_URL.replace('/v1', '')}/health") as resp:
                if resp.status == 200:
                    print("‚úÖ Chat server (port 8000) is reachable")
                    chat_reachable = True
                else:
                    print(f"‚ö†Ô∏è Chat server responded with status {resp.status}")
                    chat_reachable = False
    except Exception as e:
        print(f"‚ùå Chat server (port 8000) is not reachable: {e}")
        chat_reachable = False
    
    # Test embedding server
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{VLLM_EMBEDDING_BASE_URL.replace('/v1', '')}/health") as resp:
                if resp.status == 200:
                    print("‚úÖ Embedding server (port 8001) is reachable")
                    embedding_reachable = True
                else:
                    print(f"‚ö†Ô∏è Embedding server responded with status {resp.status}")
                    embedding_reachable = False
    except Exception as e:
        print(f"‚ùå Embedding server (port 8001) is not reachable: {e}")
        embedding_reachable = False
    
    return chat_reachable, embedding_reachable

async def test_model_listing():
    """Test listing available models from vLLM servers"""
    print("\nüìã Testing Model Listing...")
    
    import aiohttp
    import json
    
    # List models from chat server
    try:
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Bearer {VLLM_CHAT_API_KEY}"}
            async with session.get(f"{VLLM_CHAT_BASE_URL}/models", headers=headers) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    models = [model["id"] for model in data.get("data", [])]
                    print(f"‚úÖ Chat server models: {models}")
                else:
                    print(f"‚ö†Ô∏è Could not list chat models (status {resp.status})")
    except Exception as e:
        print(f"‚ùå Error listing chat models: {e}")
    
    # List models from embedding server
    try:
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Bearer {VLLM_EMBEDDING_API_KEY}"}
            async with session.get(f"{VLLM_EMBEDDING_BASE_URL}/models", headers=headers) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    models = [model["id"] for model in data.get("data", [])]
                    print(f"‚úÖ Embedding server models: {models}")
                else:
                    print(f"‚ö†Ô∏è Could not list embedding models (status {resp.status})")
    except Exception as e:
        print(f"‚ùå Error listing embedding models: {e}")

async def main():
    """Run all tests"""
    print("üöÄ Starting vLLM Functions Test Suite")
    print("=" * 60)
    
    # Print configuration
    print("üîß Configuration:")
    print(f"   Chat server: {VLLM_CHAT_BASE_URL}")
    print(f"   Chat model: {VLLM_CHAT_MODEL}")
    print(f"   Embedding server: {VLLM_EMBEDDING_BASE_URL}")
    print(f"   Embedding model: {VLLM_EMBEDDING_MODEL}")
    print("=" * 60)
    
    # Check server connectivity first
    chat_reachable, embedding_reachable = await test_server_connectivity()
    
    if not chat_reachable or not embedding_reachable:
        print("\n‚ùå Server connectivity issues detected!")
        print("\nüí° To start vLLM servers:")
        print("   # For chat model on port 8000:")
        print("   vllm serve <your-chat-model> --port 8000")
        print("   # For embedding model on port 8001:")
        print("   vllm serve <your-embedding-model> --port 8001")
        return
    
    # List available models
    await test_model_listing()
    
    tests_passed = 0
    total_tests = 4
    
    # Run functionality tests
    if await test_llm_function():
        tests_passed += 1
    
    if await test_embedding_function():
        tests_passed += 1
        
    if await test_embedding_with_custom_function():
        tests_passed += 1
        
    if await test_llm_with_system_prompt():
        tests_passed += 1
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä TEST SUMMARY")
    print(f"‚úÖ Tests passed: {tests_passed}/{total_tests}")
    
    if tests_passed == total_tests:
        print("üéâ All tests passed! Your vLLM endpoints are working correctly.")
        print("üí° You can now use these functions with LightRAG.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Please check your vLLM server configuration.")
        print("\nüí° Troubleshooting tips:")
        print("   1. Make sure both vLLM servers are running")
        print("   2. Verify the model names match your deployed models")
        print("   3. Check if the ports (8000, 8001) are correct")
        print("   4. Ensure models support the required endpoints")

if __name__ == "__main__":
    print("‚ö†Ô∏è  IMPORTANT: Update the model names before running!")
    print(f"   Current chat model: '{VLLM_CHAT_MODEL}'")
    print(f"   Current embedding model: '{VLLM_EMBEDDING_MODEL}'")
    print("   Edit the script to set your actual model names.\n")
    
    asyncio.run(main())



import os
import asyncio
import numpy as np
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

# Setup logging for LightRAG
setup_logger("lightrag", level="INFO")

WORKING_DIR = "./lightrag_storage"

# vLLM Configuration
VLLM_CHAT_BASE_URL = "http://localhost:8000/v1"
VLLM_CHAT_API_KEY = "EMPTY"  # vLLM doesn't require real API key for local servers
VLLM_CHAT_MODEL = "your-chat-model-name"  # Replace with your actual chat model name

VLLM_EMBEDDING_BASE_URL = "http://localhost:8001/v1"
VLLM_EMBEDDING_API_KEY = "EMPTY"  # vLLM doesn't require real API key for local servers  
VLLM_EMBEDDING_MODEL = "your-embedding-model-name"  # Replace with your actual embedding model name
EMBEDDING_DIMENSION = 768  # Update this to match your embedding model's dimension

# Ensure working directory exists
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -> str:
    """LLM function using vLLM chat endpoint"""
    return await openai_complete_if_cache(
        model=VLLM_CHAT_MODEL,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=VLLM_CHAT_API_KEY,
        base_url=VLLM_CHAT_BASE_URL,
        **kwargs
    )

async def embedding_func(texts: list[str]) -> np.ndarray:
    """Embedding function using vLLM embedding endpoint"""
    return await openai_embed(
        texts,
        model=VLLM_EMBEDDING_MODEL,
        api_key=VLLM_EMBEDDING_API_KEY,
        base_url=VLLM_EMBEDDING_BASE_URL
    )

async def initialize_rag():
    """Initialize LightRAG with vLLM endpoints"""
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=EMBEDDING_DIMENSION,  # Update this to match your embedding model
            max_token_size=8192,
            func=embedding_func
        )
    )
    await rag.initialize_storages()
    await initialize_pipeline_status()
    return rag

async def test_vllm_connectivity():
    """Test if vLLM servers are reachable"""
    print("üîç Testing vLLM server connectivity...")
    
    import aiohttp
    
    # Test chat server
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{VLLM_CHAT_BASE_URL.replace('/v1', '')}/health") as resp:
                if resp.status == 200:
                    print("‚úÖ Chat server (port 8000) is reachable")
                else:
                    print(f"‚ö†Ô∏è Chat server responded with status {resp.status}")
                    return False
    except Exception as e:
        print(f"‚ùå Chat server not reachable: {e}")
        return False
    
    # Test embedding server
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{VLLM_EMBEDDING_BASE_URL.replace('/v1', '')}/health") as resp:
                if resp.status == 200:
                    print("‚úÖ Embedding server (port 8001) is reachable")
                else:
                    print(f"‚ö†Ô∏è Embedding server responded with status {resp.status}")
                    return False
    except Exception as e:
        print(f"‚ùå Embedding server not reachable: {e}")
        return False
    
    return True

async def main():
    """Main function to demonstrate LightRAG usage with vLLM"""
    try:
        # Print configuration
        print("üöÄ Starting LightRAG with vLLM")
        print("=" * 50)
        print(f"Chat server: {VLLM_CHAT_BASE_URL}")
        print(f"Chat model: {VLLM_CHAT_MODEL}")
        print(f"Embedding server: {VLLM_EMBEDDING_BASE_URL}")
        print(f"Embedding model: {VLLM_EMBEDDING_MODEL}")
        print(f"Working directory: {WORKING_DIR}")
        print("=" * 50)
        
        # Test server connectivity
        if not await test_vllm_connectivity():
            print("\n‚ùå Cannot connect to vLLM servers!")
            print("\nüí° Make sure your vLLM servers are running:")
            print("   # For chat model:")
            print("   vllm serve <your-chat-model> --port 8000")
            print("   # For embedding model:")
            print("   vllm serve <your-embedding-model> --port 8001")
            return
        
        # Initialize RAG instance
        print("\nüîß Initializing LightRAG...")
        rag = await initialize_rag()
        print("‚úÖ LightRAG initialized successfully!")
        
        # Insert some sample text
        sample_text = """
        Python is a high-level, interpreted programming language created by Guido van Rossum 
        and first released in 1991. It's widely used for web development, data science, 
        artificial intelligence, machine learning, automation, and scientific computing.
        
        Key features of Python include:
        - Simple and readable syntax
        - Extensive standard library
        - Large ecosystem of third-party packages
        - Cross-platform compatibility
        - Strong community support
        
        Popular Python frameworks include Django and Flask for web development, 
        TensorFlow and PyTorch for machine learning, and pandas and NumPy for data analysis.
        """
        
        print("\nüìù Inserting sample text...")
        rag.insert(sample_text)
        print("‚úÖ Text inserted successfully!")
        
        # Perform different types of queries
        query = "What is Python used for and what are its key features?"
        
        print(f"\n‚ùì Query: {query}")
        print("\n" + "="*50)
        
        # Hybrid search (recommended)
        print("üîç Hybrid Search:")
        result = await rag.query(query, param=QueryParam(mode="hybrid"))
        print(f"üìù {result}")
        
        print("\n" + "-"*30)
        print("üîç Local Search:")
        result = await rag.query(query, param=QueryParam(mode="local"))
        print(f"üìù {result}")
        
        print("\n" + "-"*30)
        print("üîç Global Search:")
        result = await rag.query(query, param=QueryParam(mode="global"))
        print(f"üìù {result}")
        
        print("\n" + "-"*30)
        print("üîç Mix Search:")
        result = await rag.query(query, param=QueryParam(mode="mix"))
        print(f"üìù {result}")
        
    except Exception as e:
        print(f"‚ùå An error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'rag' in locals():
            await rag.finalize_storages()
            print("\n‚úÖ LightRAG finalized successfully!")

if __name__ == "__main__":
    # Check configuration
    if VLLM_CHAT_MODEL == "your-chat-model-name" or VLLM_EMBEDDING_MODEL == "your-embedding-model-name":
        print("‚ö†Ô∏è  IMPORTANT: Update the model names before running!")
        print(f"   Edit VLLM_CHAT_MODEL: '{VLLM_CHAT_MODEL}'")
        print(f"   Edit VLLM_EMBEDDING_MODEL: '{VLLM_EMBEDDING_MODEL}'")
        print(f"   Edit EMBEDDING_DIMENSION: {EMBEDDING_DIMENSION}")
        print("   Set these to match your deployed vLLM models.")
        exit(1)
    
    asyncio.run(main())
