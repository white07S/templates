"""
Minimal example of vLLM with FastAPI for OpenAI-compatible chat completions
"""

import asyncio
import json
import time
import uuid
from typing import List
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.sampling_params import SamplingParams

# Global engine
engine = None

# Request/Response models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 1.0
    max_tokens: int = 100
    stream: bool = False

@asynccontextmanager
async def lifespan(app: FastAPI):
    global engine
    
    # Initialize vLLM engine
    engine_args = AsyncEngineArgs(
        model="microsoft/phi-2",  # Use a smaller model for testing
        trust_remote_code=True,
        dtype="auto",
    )
    engine = AsyncLLMEngine.from_engine_args(engine_args)
    
    yield
    
    # Cleanup
    del engine

app = FastAPI(lifespan=lifespan)

def format_prompt(messages: List[ChatMessage]) -> str:
    """Simple chat template"""
    prompt = ""
    for msg in messages:
        prompt += f"{msg.role}: {msg.content}\n"
    prompt += "assistant:"
    return prompt

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    if not engine:
        raise HTTPException(500, "Engine not initialized")
    
    prompt = format_prompt(request.messages)
    sampling_params = SamplingParams(
        temperature=request.temperature,
        max_tokens=request.max_tokens,
    )
    
    request_id = f"req-{uuid.uuid4().hex}"
    
    if request.stream:
        async def generate():
            async for output in engine.generate(prompt, sampling_params, request_id):
                text = output.outputs[0].text
                chunk = {
                    "choices": [{
                        "delta": {"content": text[len(text) - len(output.outputs[0].delta):]},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk)}\n\n"
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(generate(), media_type="text/event-stream")
    else:
        # Non-streaming
        final_output = None
        async for output in engine.generate(prompt, sampling_params, request_id):
            final_output = output
        
        return {
            "id": f"chatcmpl-{uuid.uuid4().hex}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": final_output.outputs[0].text
                },
                "finish_reason": "stop"
            }]
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


"""
Example of how to use the vLLM FastAPI server with OpenAI client
"""

from openai import OpenAI
import requests

# Create OpenAI client pointing to your vLLM server
client = OpenAI(
    api_key="EMPTY",  # vLLM doesn't require an API key
    base_url="http://localhost:8000/v1"
)

# Example 1: Non-streaming chat completion
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",  # Should match the model loaded in your server
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ],
    temperature=0.7,
    max_tokens=100
)

print("Non-streaming response:")
print(response.choices[0].message.content)
print(f"\nUsage: {response.usage}")

# Example 2: Streaming chat completion
print("\n\nStreaming response:")
stream = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "user", "content": "Tell me a short story about a robot."}
    ],
    temperature=0.8,
    max_tokens=200,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()

# Example 3: Using vLLM-specific parameters
response = client.chat.completions.create(
    model="meta-llama/Llama-2-7b-chat-hf",
    messages=[
        {"role": "user", "content": "List three colors."}
    ],
    temperature=0.5,
    top_k=50,  # vLLM-specific parameter
    presence_penalty=0.1,
    frequency_penalty=0.1,
    max_tokens=50
)

print("\n\nResponse with vLLM-specific parameters:")
print(response.choices[0].message.content)

# Example 4: Direct HTTP request without OpenAI client
print("\n\nDirect HTTP request:")
response = requests.post(
    "http://localhost:8000/v1/chat/completions",
    json={
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "messages": [
            {"role": "user", "content": "Hello, how are you?"}
        ],
        "temperature": 0.7,
        "max_tokens": 50
    }
)

if response.status_code == 200:
    result = response.json()
    print(result["choices"][0]["message"]["content"])
else:
    print(f"Error: {response.status_code} - {response.text}")

# Example 5: List available models
models_response = requests.get("http://localhost:8000/v1/models")
if models_response.status_code == 200:
    models = models_response.json()
    print("\n\nAvailable models:")
    for model in models["data"]:
        print(f"- {model['id']}")

import asyncio
import json
import time
import uuid
from typing import Dict, List, Optional, Union, AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field

from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.sampling_params import SamplingParams
from vllm.lora.request import LoRARequest


# OpenAI-compatible request/response models
class ChatMessage(BaseModel):
    role: str
    content: str


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    presence_penalty: Optional[float] = 0.0
    frequency_penalty: Optional[float] = 0.0
    logit_bias: Optional[Dict[str, float]] = None
    user: Optional[str] = None
    # Additional vLLM-specific parameters
    best_of: Optional[int] = None
    top_k: Optional[int] = -1
    ignore_eos: Optional[bool] = False
    use_beam_search: Optional[bool] = False


class ChatChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Optional[str] = None


class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatChoice]
    usage: Optional[Dict[str, int]] = None


class DeltaMessage(BaseModel):
    role: Optional[str] = None
    content: Optional[str] = None


class ChatCompletionChunk(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion.chunk"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[Dict[str, Union[int, DeltaMessage, str, None]]]


# Global engine instance
engine: Optional[AsyncLLMEngine] = None


def apply_chat_template(
    messages: List[ChatMessage],
    add_generation_prompt: bool = True
) -> str:
    """
    Apply a simple chat template. In production, you should use the
    tokenizer's chat template for the specific model you're using.
    """
    formatted_messages = []
    for message in messages:
        if message.role == "system":
            formatted_messages.append(f"System: {message.content}")
        elif message.role == "user":
            formatted_messages.append(f"User: {message.content}")
        elif message.role == "assistant":
            formatted_messages.append(f"Assistant: {message.content}")
    
    prompt = "\n\n".join(formatted_messages)
    if add_generation_prompt:
        prompt += "\n\nAssistant:"
    
    return prompt


async def stream_chat_completion(
    request: ChatCompletionRequest,
    request_id: str
) -> AsyncGenerator[str, None]:
    """Generate streaming chat completion responses."""
    # Convert messages to prompt
    prompt = apply_chat_template(request.messages)
    
    # Create sampling parameters
    sampling_params = SamplingParams(
        n=request.n,
        temperature=request.temperature,
        top_p=request.top_p,
        top_k=request.top_k,
        presence_penalty=request.presence_penalty,
        frequency_penalty=request.frequency_penalty,
        max_tokens=request.max_tokens,
        logit_bias=request.logit_bias,
        ignore_eos=request.ignore_eos,
        use_beam_search=request.use_beam_search,
        best_of=request.best_of,
    )
    
    # Generate responses
    results_generator = engine.generate(prompt, sampling_params, request_id)
    
    # Stream the responses
    async for request_output in results_generator:
        if request_output.finished:
            # Final chunk
            for i, output in enumerate(request_output.outputs):
                chunk = ChatCompletionChunk(
                    model=request.model,
                    choices=[{
                        "index": i,
                        "delta": DeltaMessage(),
                        "finish_reason": output.finish_reason.value if output.finish_reason else None
                    }]
                )
                yield f"data: {chunk.model_dump_json()}\n\n"
        else:
            # Intermediate chunks
            for i, output in enumerate(request_output.outputs):
                # Get only the new text generated in this iteration
                text = output.text[len(output.text) - len(output.delta):]
                chunk = ChatCompletionChunk(
                    model=request.model,
                    choices=[{
                        "index": i,
                        "delta": DeltaMessage(content=text),
                        "finish_reason": None
                    }]
                )
                yield f"data: {chunk.model_dump_json()}\n\n"
    
    yield "data: [DONE]\n\n"


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize and cleanup the vLLM engine."""
    global engine
    
    # Initialize engine arguments
    # You can customize these based on your needs
    engine_args = AsyncEngineArgs(
        model="meta-llama/Llama-2-7b-chat-hf",  # Change to your model
        trust_remote_code=True,
        dtype="auto",
        gpu_memory_utilization=0.9,
        max_model_len=4096,
        # Add other arguments as needed
    )
    
    # Create the engine
    engine = AsyncLLMEngine.from_engine_args(engine_args)
    
    yield
    
    # Cleanup
    if engine is not None:
        del engine


# Create FastAPI app
app = FastAPI(title="vLLM API Server", lifespan=lifespan)


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """OpenAI-compatible chat completions endpoint."""
    if engine is None:
        raise HTTPException(status_code=500, detail="Engine not initialized")
    
    # Generate a unique request ID
    request_id = f"chatcmpl-{uuid.uuid4().hex}"
    
    try:
        if request.stream:
            # Streaming response
            generator = stream_chat_completion(request, request_id)
            return StreamingResponse(
                generator,
                media_type="text/event-stream"
            )
        else:
            # Non-streaming response
            prompt = apply_chat_template(request.messages)
            
            sampling_params = SamplingParams(
                n=request.n,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                presence_penalty=request.presence_penalty,
                frequency_penalty=request.frequency_penalty,
                max_tokens=request.max_tokens,
                logit_bias=request.logit_bias,
                ignore_eos=request.ignore_eos,
                use_beam_search=request.use_beam_search,
                best_of=request.best_of,
            )
            
            # Generate the full response
            results_generator = engine.generate(prompt, sampling_params, request_id)
            
            # Collect all outputs
            final_output = None
            async for request_output in results_generator:
                final_output = request_output
            
            # Format the response
            choices = []
            for i, output in enumerate(final_output.outputs):
                choices.append(ChatChoice(
                    index=i,
                    message=ChatMessage(
                        role="assistant",
                        content=output.text
                    ),
                    finish_reason=output.finish_reason.value if output.finish_reason else None
                ))
            
            # Calculate usage statistics
            usage = {
                "prompt_tokens": len(final_output.prompt_token_ids),
                "completion_tokens": sum(len(output.token_ids) for output in final_output.outputs),
                "total_tokens": len(final_output.prompt_token_ids) + sum(len(output.token_ids) for output in final_output.outputs)
            }
            
            response = ChatCompletionResponse(
                model=request.model,
                choices=choices,
                usage=usage
            )
            
            return response
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/models")
async def list_models():
    """List available models."""
    if engine is None:
        raise HTTPException(status_code=500, detail="Engine not initialized")
    
    # In a real implementation, you might want to track loaded models
    # For now, we'll return the model from engine args
    model_name = engine.engine_args.model
    
    return {
        "object": "list",
        "data": [
            {
                "id": model_name,
                "object": "model",
                "created": int(time.time()),
                "owned_by": "vllm",
                "permission": [],
                "root": model_name,
                "parent": None,
            }
        ]
    }


@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy"}


if __name__ == "__main__":
    import uvicorn
    
    # Run the server
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
