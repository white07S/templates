We are designing a task runner backend and scripts to run, track, and manage LLM tasks.

for adding any lib: use uv add "lib name"
requirements:

1: user uploads a file(excel/csv) , choose a task and data_case and submit it.

Example:
{
  "file": "path/to/uploaded/file.csv",
  "task": "summarize",
  "data_case": "exterrnal_loss"
}

internally we maintain a config file: config.json
config file is as follows:

{
  "tasks": {
    "summarize": {
      "description": "Summarize the content of the uploaded file.",
      "prompt": "summary_prompt.py",
      "data_cases": {
        "external_loss": {
          "description": "This is a loss description outside of UBS company",
          "mandatory_columns": {
            "DescriptionOfEvent": "This column represents the description of various events.",
            "nfr_taxonomy": "This column represents the non-financial risk taxonomy."
          }
        },
        "internal_loss": {
          "description": "This is a loss description within UBS company",
          "mandatory_columns": {
            "DescriptionOfEvent": "This column represents the description of various events.",
            "nfr_taxonomy": "This column represents the non-financial risk taxonomy."
          }
        }
      },
    }
  }
}


now summary_prompt.py is rendered based on config.json

def render_summary_prompt(data_case, row):

    '''
        # We define a persona
    # we define the task
    here we explain also about data from config description.
    # we define data points to be used:
    #. in this case we are passing like this:
    we pass one row at a time with all the columns
    Data Points:

    1: DescriptionOfEvent: {row['DescriptionOfEvent']}
    2: nfr_taxonomy: {row['nfr_taxonomy']}

    then we define some instructions for the LLM to follow

    like <chain-of-thought> or <step-by-step> or <reasoning> etc.
    # we define the output format

    first we explain the output format in pydantic schema format,

    then we explain in json format, then we explain naturally.

    review this: https://python.langchain.com/docs/concepts/structured_outputs/
    '''

each task has a prompt defined in prompt-lib folder


once the prompt is rendered we send it to the LLM for processing.
implement spmething like this: 
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=(
        retry_if_exception_type(RateLimitError)
        | retry_if_exception_type(APIConnectionError)
        | retry_if_exception_type(APITimeoutError)
        | retry_if_exception_type(InvalidResponseError)
    ),
) really nice code sample here if you want to use it: https://github.com/HKUDS/LightRAG/blob/main/lightrag/llm/openai.py


we want to use multiprocessing and threading for faster performance and rather than sending each task one by one.

now lets talk about what we are doing with processed data:

we maintain a database which is file based using duckdb and tinydb.

Here is how it looks like:

/external_loss
    /processed
        /hash_description_of_event
            /meta_data
                details.json (all the columns values which are not generated by ai it was already there.)
            /ai_results
                summary.json
                learning_outcome.json
                etc.
        we do this for all event.

    /to_process
        /hash_description_of_event
            /meta_data
                details.json (all the columns values which are not generated by ai it was already there.)
            there will be no ai_results here, this is for tasks which are yet to be processed.


when a user uploads a file , first we check if mandatory_columns are present , then  we check what we have already processed and what is still to be processed. based on that we create to_process files dir. 

we maintain a fast file based cache for each llm call separate from the main database to speed up the read and write and if program crashes we can recover the state easily and dont have to start from again.

I want to two different versions of using the code:

Important note: we need to keep the file user uploaded as needed later to send back. 
API version: where user can upload the file and get a task_id and then he can check with task_id to get the status and results of the processing. (we only show pending, processing, done), once the task id done, we create new column based on task (like in this case summary), it add summary for all the records and downlaods the file. 
Script version: we user a command line to --file , --task --data_case (here tqdm will show progress), one script for runnnig, and one script for checking status  and download once its done baesed on task id.,

 given we are using tqdm we can show approx time when task turns into processing.


create a mock csv file to test the script. 

for now we are assuming that this scripts are running immediately.


I want proper performance optimization, error and exception handling, and logging.

tech stack: python, fastapi, pydantic, duckdb, tinydb, tqdm to manage progress bars and display processing status.