We are creating a search algorithm.   Lib to use: tantivy python, faiss-cpu, multiprocessing or threading.  advanced fast search.  A python file for config: DATA_PATH: PATH TO PARQUET FILE. INDEX_PATH: WHERE INDEX WILL BE STORED. EMBEDDINGS_COLUMNS = ["COLUMNS NAME", "COLUMN NAME 2] VECTOR DIMENSIONS =  OPENAI_API_KEY= OPENAI_MODEL= REINDEX=TRUE OR FALSE. CHUNK_SIZE = 1000 PARALLEL WORKER ETC. TANTIVY_HEAP_SIZE=FILE SIZE * 4  Requirement: IN THE PARQUET FILE I HAVE MIN 30 COLUMNS, MAX UPTO 100, COLUMNS CAN HAVE STRING, INT, FLOAT, DATA, NULL, EMPTY STRING, NAN, OTHER EDGE CASES.  WE WANT TWO SEARCH MODE:  1: KEYWORDS: TAKES LIST OF STRING OR LIST OF SENTENCES AND SEARCH IN EACH COLUMNS FOR BEST MATCH AND RETURN THE WHOLE ROWS , RANK BASED ON RRF , IMPLEMENT ACCURATE MATCHING AND RANKING. IT HAS TO FAST. 2: SEMANTIC: TAKES STR AS INPUT, WE COMPUTE EMBEDDINGS AND MATCH IN EITHER OF THE EMBEDDINGS_COLUMNS AND RETURN TOP 5 , USE THIS: faiss.METRIC_L2. 3: COMBINED: LIST OF STR AS INPUT, WE CALL BOTH EMBEDDINGS AND SEMANTIC AND RETURN BOTH, RERANK USING RRF.   CREATE INGESTION.PY, SEARCH.PY , CONFIG.PY AND ENGINE.PY, UTILS.PY : IN UTILS CREATE OPTMIZED ASYNC OPEN AI EMBEDDING CLIENT WHICH USES ASYNC WITH BATCHING FOR FASTER INDEXING AND INGESTION. STORE CACHE OF EMBEDDINGS CALLS USING HASH OF INPUT SUCH THAT EVEN IF REINDEX HAPPEN IN FUTURE, FOR THOSE WHERE CACHE DONT CALL API.  USE TQDM TO SHOW PROGRESS OF INDEXING AND INGESTION, CLEAR IMPLEMENTATION TO SEE THE PROGRESS.  THE SPEED HAS TO BE LESS THAN 150MS FOR SEARCH.   IGNORE UPPERCASE OF MY WRITING, YOU CAN BROWSE INTERNET FOR UPDATED INFO, please check for updated docs before you write any code.. YOU CAN USE UV ADD LIB TO INSTALL SOMETHING.

[!CAUTION: WHEN RETURNING THE RESULTS, WE WANT TO RETURN MATCHED COMPLETE ROWS. IMAGINE, IF SOMETHING MATCHED ON ROW 10, ROW 111, ROW 16, ROW 12, WE RETURN ALL ROWS WITH ALL ITS COLUMNS AS DICT , LIST OF DICT.]