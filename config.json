# pip install mem0 langchain langchain-openai faiss-cpu kuzu-python

import os
from mem0 import Memory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # swap to your LC providers if needed

# --- ENV (example: OpenAI via LangChain) ---
os.environ["OPENAI_API_KEY"] = "sk-..."  # or set your provider-specific envs

# --- LangChain models (use *any* LC chat/embedding class you prefer) ---
lc_chat = ChatOpenAI(model="gpt-4o", temperature=0.2, max_tokens=2000)
lc_embed = OpenAIEmbeddings(model="text-embedding-3-small", dimensions=1536)

# --- Mem0 config: LangChain LLM + LangChain Embeddings + FAISS + Kùzu Graph Store ---
config = {
    "llm": {
        "provider": "langchain",
        "config": {
            "model": lc_chat
        }
    },
    "embedder": {
        "provider": "langchain",
        "config": {
            "model": lc_embed
        }
    },
    "vector_store": {
        "provider": "faiss",
        "config": {
            "collection_name": "mem0_demo",
            "path": "/tmp/faiss_memories",       # persisted FAISS index dir
            "distance_strategy": "cosine"        # or "euclidean", "inner_product"
            # "normalize_L2": True                # optional for euclidean usage
        }
    },
    "graph_store": {
        "provider": "kuzu",
        "config": {
            "db": "mem0_graph.kuzu"              # on-disk; use ":memory:" for in-memory ephemeral
        }
        # Optional: override graph-only LLM (else the main llm is used)
        # "llm": {
        #     "provider": "langchain",
        #     "config": { "model": lc_chat }
        # }
    }
}

# --- Use it ---
m = Memory.from_config(config)

# Example add/search
messages = [
    {"role": "user", "content": "I live in Kraków and prefer sci-fi movies."}
]
m.add(messages, user_id="alice", metadata={"domain": "prefs"})
print(m.search("movie genre preference?", user_id="alice"))
